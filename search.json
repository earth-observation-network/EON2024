[
  {
    "objectID": "ml_session/ML_AOA.html",
    "href": "ml_session/ML_AOA.html",
    "title": "Machine learning for remote sensing applications",
    "section": "",
    "text": "In this tutorial we will go through the basic workflow of training machine learning models for spatial mapping based on remote sensing. To do this we will look at two case studies located in the MarburgOpenForest in Germany: one has the aim to produce a land cover map including different tree species; the other aims at producing a map of Leaf Area Index.\nBased on “default” models, we will further discuss the relevance of different validation strategies and the area of applicability.\n\n\nFor this tutorial we need the raster package for processing of the satellite data (note: needs to be replaced by terra soon) as well as the caret package as a wrapper for machine learning (here: randomForest) algorithms. Sf is used for handling of the training data available as vector data (polygons). Mapview is used for spatial visualization of the data. CAST will be used to account for spatial dependencies during model validation as well as for the estimation of the AOA.\n\nrm(list=ls())\n#major required packages:\n\nrequire(devtools)\ndevtools::install_github(\"HannaMeyer/CAST\")\n\nlibrary(raster)\nlibrary(terra)\nlibrary(caret)\nlibrary(randomForest)\nlibrary(twosamples)\nlibrary(mapview)\nlibrary(sf)\nlibrary(CAST)\nlibrary(tmap)\nlibrary(rprojroot)\n\n\n# create a string containing the current working directory\nwd = paste0(find_rstudio_root_file(),\"/ml_session/data/\")",
    "crumbs": [
      "Machine Learning",
      "Machine learning for remote sensing applications"
    ]
  },
  {
    "objectID": "ml_session/ML_AOA.html#how-to-start",
    "href": "ml_session/ML_AOA.html#how-to-start",
    "title": "Machine learning for remote sensing applications",
    "section": "",
    "text": "For this tutorial we need the raster package for processing of the satellite data (note: needs to be replaced by terra soon) as well as the caret package as a wrapper for machine learning (here: randomForest) algorithms. Sf is used for handling of the training data available as vector data (polygons). Mapview is used for spatial visualization of the data. CAST will be used to account for spatial dependencies during model validation as well as for the estimation of the AOA.\n\nrm(list=ls())\n#major required packages:\n\nrequire(devtools)\ndevtools::install_github(\"HannaMeyer/CAST\")\n\nlibrary(raster)\nlibrary(terra)\nlibrary(caret)\nlibrary(randomForest)\nlibrary(twosamples)\nlibrary(mapview)\nlibrary(sf)\nlibrary(CAST)\nlibrary(tmap)\nlibrary(rprojroot)\n\n\n# create a string containing the current working directory\nwd = paste0(find_rstudio_root_file(),\"/ml_session/data/\")",
    "crumbs": [
      "Machine Learning",
      "Machine learning for remote sensing applications"
    ]
  },
  {
    "objectID": "ml_session/ML_AOA.html#data-preparation",
    "href": "ml_session/ML_AOA.html#data-preparation",
    "title": "Machine learning for remote sensing applications",
    "section": "Data preparation",
    "text": "Data preparation\nTo start with, let’s load and explore the remote sensing raster data as well as the vector data that include the training sites.\n\nRaster data (predictor variables)\n\nmof_sen &lt;- rast(paste0(wd,\"sentinel_uniwald.grd\"))\nprint(mof_sen)\n\nclass       : SpatRaster \ndimensions  : 522, 588, 10  (nrow, ncol, nlyr)\nresolution  : 10, 10  (x, y)\nextent      : 474200, 480080, 5629540, 5634760  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=utm +zone=32 +datum=WGS84 +units=m +no_defs \nsource      : sentinel_uniwald.grd \nnames       : T32UM~1_B02, T32UM~1_B03, T32UM~1_B04, T32UM~1_B05, T32UM~1_B06, T32UM~1_B07, ... \nmin values  :         723,         514,         294,    341.8125,    396.9375,    440.8125, ... \nmax values  :        8325,        9087,       13810,   7368.7500,   8683.8125,   9602.3125, ... \n\n\nThe raster data contain a subset of the optical data from Sentinel-2 (see band information here: https://en.wikipedia.org/wiki/Sentinel-2) given in scaled reflectances (B02-B11). In addition,the NDVI was calculated. Let’s plot the data to get an idea how the variables look like.\n\nplot(mof_sen)\n\n\n\n\n\n\n\nplotRGB(mof_sen,r=3,g=2,b=1,stretch=\"lin\")\n\n\n\n\n\n\n\n\n\n\nVector data (Response variable)\nThe vector file is read as sf object. It contains the training sites that will be regarded here as a ground truth for the land cover classification.\n\ntrainSites &lt;- read_sf(paste0(wd,\"trainingsites_LUC.gpkg\"))\n\nUsing mapview we can visualize the aerial image channels in the geographical context and overlay it with the polygons. Click on the polygons to see which land cover class is assigned to a respective polygon.\n\nmapview(raster(mof_sen[[1]]), map.types = \"Esri.WorldImagery\") +\n  mapview(trainSites)\n\n\n\n\n\n\n\nDraw training samples and extract raster information\nIn order to train a machine learning model between the spectral properties and the land cover class, we first need to create a data frame that contains the predictor variables at the location of the training sites as well as the corresponding class information. However, using each pixel overlapped by a polygon would lead to a overly huge dataset, therefore, we first draw training samples from the polygon. Let’s use 1000 randomly sampled (within the polygons) pixels as training data set.\n\ntrainlocations &lt;- st_sample(trainSites,1000)\ntrainlocations &lt;- st_join(st_sf(trainlocations), trainSites)\nmapview(trainlocations)\n\n\n\n\n\nNext, we can extract the raster values for these locations. The resulting data frame contains the predictor variables for each training location that we can merged with the information on the land cover class from the sf object.\n\ntrainDat &lt;- extract(mof_sen, trainlocations, df=TRUE)\ntrainDat &lt;- data.frame(trainDat, trainlocations)\nhead(trainDat)\n\n  ID T32UMB_20170510T103031_B02 T32UMB_20170510T103031_B03\n1  1                        827                        836\n2  2                        822                        772\n3  3                        804                        799\n4  4                        822                        780\n5  5                        927                        922\n6  6                        861                        795\n  T32UMB_20170510T103031_B04 T32UMB_20170510T103031_B05\n1                        508                  1074.0000\n2                        432                  1010.1875\n3                        482                  1061.3750\n4                        443                   923.5625\n5                        630                  1214.1875\n6                        440                   967.4375\n  T32UMB_20170510T103031_B06 T32UMB_20170510T103031_B07\n1                   1892.250                   2141.875\n2                   1834.062                   2123.000\n3                   1972.500                   2250.688\n4                   3367.062                   4825.688\n5                   2930.188                   3635.312\n6                   3283.312                   4301.812\n  T32UMB_20170510T103031_B08 T32UMB_20170510T103031_B11\n1                       2260                   1427.188\n2                       2007                   1491.000\n3                       2383                   1463.375\n4                       4781                   1586.688\n5                       3616                   2042.812\n6                       4276                   1616.125\n  T32UMB_20170510T103031_B12      NDVI id  LN  Type           trainlocations\n1                   712.0000 0.6329480 NA   2 Buche POINT (477095.6 5631952)\n2                   767.3125 0.6457565 NA   2 Buche POINT (477395.5 5632144)\n3                   727.8125 0.6635253 NA   2 Buche POINT (477301.3 5631712)\n4                   667.3750 0.8303981 65 303 Wiese POINT (476103.6 5631700)\n5                   975.3125 0.7032501 81 305 Wiese POINT (477712.4 5632182)\n6                   670.8750 0.8134012 37 303 Wiese POINT (476314.4 5632947)",
    "crumbs": [
      "Machine Learning",
      "Machine learning for remote sensing applications"
    ]
  },
  {
    "objectID": "ml_session/ML_AOA.html#model-training",
    "href": "ml_session/ML_AOA.html#model-training",
    "title": "Machine learning for remote sensing applications",
    "section": "Model training",
    "text": "Model training\n\nPredictors and response\nFor model training we need to define the predictor and response variables. As predictors we can use basically all information from the raster stack as we might assume they could all be meaningful for the differentiation between the land cover classes. As response variable we use the “Label” column of the data frame.\n\npredictors &lt;- names(mof_sen)\nresponse &lt;- \"Type\"\n\n\n\nA first “default” model\nWe then train a Random Forest model to lean how the classes can be distinguished based on the predictors (note: other algorithms would work as well. See https://topepo.github.io/caret/available-models.html for a list of algorithms available in caret). Caret’s train function is doing this job.\nSo let’s see how we can then train a “default” random forest model. We specify “rf” as method, indicating that a Random Forest is applied. We reduce the number of trees (ntree) to 75 to speed things up. Note that usually a larger number (&gt;250) is appropriate.\n\nmodel &lt;- train(trainDat[,predictors],\n               trainDat[,response],\n               method=\"rf\",\n               ntree=75)\nmodel\n\nRandom Forest \n\n1000 samples\n  10 predictor\n  10 classes: 'Buche', 'Duglasie', 'Eiche', 'Felder', 'Fichte', 'Laerche', 'Siedlung', 'Strasse', 'Wasser', 'Wiese' \n\nNo pre-processing\nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 1000, 1000, 1000, 1000, 1000, 1000, ... \nResampling results across tuning parameters:\n\n  mtry  Accuracy   Kappa    \n   2    0.8266168  0.7853027\n   6    0.8249949  0.7837342\n  10    0.8186689  0.7760377\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 2.\n\n\nTo perform the classification we can then use the trained model and apply it to each pixel of the raster stack using the predict function.\n\nprediction &lt;- predict(mof_sen,model)\n\nThen we can then create a map with meaningful colors of the predicted land cover using the tmap package.\n\ncols &lt;- rev(c(\"palegreen\", \"blue\", \"grey\", \"red\", \"lightgreen\", \"forestgreen\", \"beige\",\"brown\",\"darkgreen\",\"yellowgreen\"))\n\ntm_shape(prediction) +\n  tm_raster(palette = cols,title = \"LUC\")+\n  tm_scale_bar(bg.color=\"white\",bg.alpha=0.75)+\n  tm_layout(legend.bg.color = \"white\",\n            legend.bg.alpha = 0.75)\n\n\n\n\n\n\n\n\nBased on this we can now discuss more advanced aspects of cross-validation for performance assessment as well as spatial variable selection strategies.\n\n\nModel training with spatial CV and variable selection\nBefore starting model training we can specify some control settings using trainControl. For hyperparameter tuning (mtry) as well as for error assessment we use a spatial cross-validation. Here, the training data are split into 5 folds by trying to resemble the geographic distance distribution required when predicting the entire area from the trainign data,\n\n## define prediction area:\nstudyArea &lt;- as.polygons(mof_sen, values = FALSE, na.all = TRUE) |&gt;\n    st_as_sf() |&gt;\n    st_transform(st_crs(trainlocations))|&gt;\n    st_union()\nmapview(studyArea)\n\n\n\n\n indices &lt;- knndm(trainlocations,studyArea,k=5)\ngd &lt;- geodist(trainlocations,studyArea,cvfolds = indices$indx_train )\nplot(gd)+ scale_x_log10(labels=round)\n\n\n\n\n\n\n\nctrl &lt;- trainControl(method=\"cv\", \n                     index = indices$indx_train,\n                     indexOut = indices$indx_test,\n                     savePredictions = TRUE)\n\nModel training is then again performed using caret’s train function. However we use a wrapper around it that is selecting the predictor variables which are relevant for making predictions to new spatial locations (forward feature selection, fss). We use the Kappa index as metric to select the best model.\n\n# train the model\nset.seed(100)\nmodel &lt;- ffs(trainDat[,predictors],\n             trainDat[,response],\n             method=\"rf\",\n             metric=\"Kappa\",\n             trControl=ctrl,\n             importance=TRUE,\n             ntree=200,\n             verbose=FALSE)\n\n\nprint(model)\n\nSelected Variables: \nT32UMB_20170510T103031_B03 T32UMB_20170510T103031_B08 T32UMB_20170510T103031_B05 T32UMB_20170510T103031_B12 T32UMB_20170510T103031_B07 T32UMB_20170510T103031_B11\n---\nRandom Forest \n\n1000 samples\n   6 predictor\n  10 classes: 'Buche', 'Duglasie', 'Eiche', 'Felder', 'Fichte', 'Laerche', 'Siedlung', 'Strasse', 'Wasser', 'Wiese' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 908, 684, 696, 871, 841 \nResampling results across tuning parameters:\n\n  mtry  Accuracy   Kappa    \n  2     0.6970762  0.5033759\n  4     0.6795524  0.4849792\n  6     0.6924207  0.5098469\n\nKappa was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 6.\n\nplot(varImp(model))\n\n\n\n\n\n\n\n\n\n\nModel validation\nWhen we print the model (see above) we get a summary of the prediction performance as the average Kappa and Accuracy of the three spatial folds. Looking at all cross-validated predictions together we can get the “global” model performance.\n\n# get all cross-validated predictions:\ncvPredictions &lt;- model$pred[model$pred$mtry==model$bestTune$mtry,]\n# calculate cross table:\ntable(cvPredictions$pred,cvPredictions$obs)\n\n          \n           Buche Duglasie Eiche Felder Fichte Laerche Siedlung Strasse Wasser\n  Buche       48        0    17      8      3       0        0       2      0\n  Duglasie     0        0     0      0      1       0        0       0      0\n  Eiche        8        0    17      0      5       0        0       0      0\n  Felder       7        0     3    250      0       0        0       6      0\n  Fichte       3        0     1      1      4       0        0       0      1\n  Laerche      2        0     1      2      0       0        0       0      0\n  Siedlung     2        0    12      0      0       0        0       0      0\n  Strasse      0        0     0     32      0       0        0      26      0\n  Wasser       7        0     2      0      1       0        0       1      2\n  Wiese        0        0     0     23      0       0        0       2      0\n          \n           Wiese\n  Buche        0\n  Duglasie     0\n  Eiche        0\n  Felder       4\n  Fichte       0\n  Laerche      1\n  Siedlung     0\n  Strasse     11\n  Wasser       0\n  Wiese       76\n\n\n\n\nVisualize the final model predictions\n\nprediction &lt;- predict(mof_sen,model)\ncols &lt;- rev(c(\"palegreen\", \"blue\", \"grey\", \"red\", \"lightgreen\", \"forestgreen\", \"beige\",\"brown\",\"darkgreen\",\"yellowgreen\"))\n\ntm_shape(prediction) +\n  tm_raster(palette = cols,title = \"LUC\")+\n  tm_scale_bar(bg.color=\"white\",bg.alpha=0.75)+\n  tm_layout(legend.bg.color = \"white\",\n            legend.bg.alpha = 0.75)",
    "crumbs": [
      "Machine Learning",
      "Machine learning for remote sensing applications"
    ]
  },
  {
    "objectID": "ml_session/ML_AOA.html#area-of-applicability",
    "href": "ml_session/ML_AOA.html#area-of-applicability",
    "title": "Machine learning for remote sensing applications",
    "section": "Area of Applicability",
    "text": "Area of Applicability\nWe have seen that technically, the trained model can be applied to the entire area of interest (and beyond…as long as the sentinel predictors are available which they are, even globally). But we should assess if we SHOULD apply our model to the entire area. The model should only be applied to locations that feature predictor properties that are comparable to those of the training data. If dissimilarity to the training data is larger than the dissimmilarity within the training data, the model should not be applied to this location.\n\nAOA &lt;- aoa(mof_sen,model)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |==                                                                    |   4%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |=========                                                             |  14%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |================                                                      |  24%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |=======================                                               |  34%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |============================                                          |  41%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |==============================                                        |  44%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |=====================================                                 |  54%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |==========================================                            |  61%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |============================================                          |  64%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |===================================================                   |  74%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |========================================================              |  81%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |==========================================================            |  84%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |===============================================================       |  91%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |=================================================================     |  94%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n\nplot(AOA$AOA)\n\n\n\n\n\n\n\n\nThe result of the aoa function has two layers: the dissimilarity index (DI) and the area of applicability (AOA). The DI can take values from 0 to Inf, where 0 means that a location has predictor properties that are identical to properties observed in the training data. With increasing values the dissimilarity increases. The AOA has only two values: 0 and 1. 0 means that a location is outside the area of applicability, 1 means that the model is inside the area of applicability.",
    "crumbs": [
      "Machine Learning",
      "Machine learning for remote sensing applications"
    ]
  },
  {
    "objectID": "ml_session/ML_AOA.html#prepare-data",
    "href": "ml_session/ML_AOA.html#prepare-data",
    "title": "Machine learning for remote sensing applications",
    "section": "Prepare data",
    "text": "Prepare data\n\nmof_sen &lt;- rast(\"data/sentinel_uniwald.grd\")\nLAIdat &lt;- st_read(\"data/trainingsites_LAI.gpkg\")\n\nReading layer `trainingsites_LAI' from data source \n  `/home/creu/edu/gisma-courses/EON2024/ml_session/data/trainingsites_LAI.gpkg' \n  using driver `GPKG'\nSimple feature collection with 67 features and 10 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 476350 ymin: 5631537 xmax: 478075 ymax: 5632765\nProjected CRS: WGS 84 / UTM zone 32N\n\ntrainDat &lt;- extract(mof_sen,LAIdat,na.rm=TRUE)\ntrainDat$LAI &lt;- LAIdat$LAI\n\n\nmeanmodel &lt;- mof_sen[[1]]\nvalues(meanmodel) &lt;- mean(trainDat$LAI)\nplot(meanmodel)\n\n\n\n\n\n\n\nrandommodel &lt;- mof_sen[[1]]\nvalues(randommodel)&lt;- runif(ncell(randommodel),min = 0,4)\n\nplot(randommodel)",
    "crumbs": [
      "Machine Learning",
      "Machine learning for remote sensing applications"
    ]
  },
  {
    "objectID": "ml_session/ML_AOA.html#a-simple-linear-model",
    "href": "ml_session/ML_AOA.html#a-simple-linear-model",
    "title": "Machine learning for remote sensing applications",
    "section": "A simple linear model",
    "text": "A simple linear model\nAs a simple first approach we might develop a linear model. Let’s assume a linear relationship between the NDVI and the LAI\n\nplot(trainDat$NDVI,trainDat$LAI)\nmodel_lm &lt;- lm(LAI~NDVI,data=trainDat)\nsummary(model_lm)\n\n\nCall:\nlm(formula = LAI ~ NDVI, data = trainDat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.87314 -0.52143 -0.03363  0.63668  2.25252 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  -0.8518     1.4732  -0.578  0.56515   \nNDVI          6.8433     2.3160   2.955  0.00435 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8887 on 65 degrees of freedom\nMultiple R-squared:  0.1184,    Adjusted R-squared:  0.1049 \nF-statistic: 8.731 on 1 and 65 DF,  p-value: 0.004354\n\nabline(model_lm,col=\"red\")\n\n\n\n\n\n\n\nprediction_LAI &lt;- predict(mof_sen,model_lm,na.rm=T)\nplot(prediction_LAI)\n\n\n\n\n\n\n\nlimodelpred &lt;- -0.8518+mof_sen$NDVI*6.8433\nmapview(raster(limodelpred))",
    "crumbs": [
      "Machine Learning",
      "Machine learning for remote sensing applications"
    ]
  },
  {
    "objectID": "ml_session/ML_AOA.html#the-machine-learning-way",
    "href": "ml_session/ML_AOA.html#the-machine-learning-way",
    "title": "Machine learning for remote sensing applications",
    "section": "The machine learning way",
    "text": "The machine learning way\n\nDefine CV folds\nLet’s use the NNDM cross-validation approach.\n\nstudyArea &lt;- as.polygons(mof_sen, values = FALSE, na.all = TRUE) |&gt;\n    st_as_sf() |&gt;\n    st_transform(st_crs(LAIdat))|&gt;\n    st_union()\n\nnndm_folds &lt;- knndm(LAIdat,studyArea,k=3)\n\nLet’s explore the geodistance\n\ngd &lt;- geodist(LAIdat,studyArea,cvfolds = nndm_folds$indx_test)\nplot(gd)\n\n\n\n\n\n\n\n\n\n\nModel training\n\nctrl &lt;- trainControl(method=\"cv\",\n                     index=nndm_folds$indx_train,\n                     indexOut = nndm_folds$indx_test,\n                    savePredictions = \"all\")\n\n\nmodel_rf &lt;- train(trainDat[,2:11],\n                  trainDat$LAI,\n                  method = \"rf\")\n\n\n\nmodel &lt;- ffs(trainDat[,predictors],\n             trainDat$LAI,\n             method=\"rf\",\n             trControl = ctrl,\n             importance=TRUE,\n             verbose=FALSE)\n\n\nmodel\n\nSelected Variables: \nT32UMB_20170510T103031_B07 T32UMB_20170510T103031_B08 NDVI\n---\nRandom Forest \n\n67 samples\n 3 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 41, 44, 49 \nResampling results across tuning parameters:\n\n  mtry  RMSE       Rsquared   MAE      \n  2     0.8457649  0.2159841  0.7052101\n  3     0.8555533  0.2109720  0.7166645\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was mtry = 2.\n\n\n\n\nLAI prediction\nLet’s then use the trained model for prediction.\n\nLAIprediction &lt;- predict(mof_sen,model)\nplot(LAIprediction)\n\n\n\n\n\n\n\n\n\nQuestion?! Why does it look so different than the linear model?\n\n\n\nAOA estimation\n\nAOA &lt;- aoa(mof_sen,model_rf)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n\nplot(AOA$AOA)",
    "crumbs": [
      "Machine Learning",
      "Machine learning for remote sensing applications"
    ]
  },
  {
    "objectID": "tdv_session/01_TrainingDataCollection.html",
    "href": "tdv_session/01_TrainingDataCollection.html",
    "title": "Collection of training data for remote sensing model building",
    "section": "",
    "text": "#install.packages(\"devtools\")\ndevtools::install_github(\"bleutner/RStoolbox\")\n\nSkipping install of 'RStoolbox' from a github remote, the SHA1 (e7587e2f) has not changed since last install.\n  Use `force = TRUE` to force installation\n\nlibrary(sf)\n\nLinking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.3.1; sf_use_s2() is TRUE\n\nlibrary(RStoolbox)\n\nThis is version 1.0.0 of RStoolbox\n\nlibrary(raster)\n\nLade nötiges Paket: sp\n\nlibrary(knitr)\nlibrary(ggplot2)\n\nlibrary(patchwork)\n\n\nAttache Paket: 'patchwork'\n\n\nDas folgende Objekt ist maskiert 'package:raster':\n\n    area\n\nlibrary(mapview)\nlibrary(kableExtra)\n\nlibrary(rprojroot)",
    "crumbs": [
      "Training Data",
      "Collection of training data for remote sensing model building"
    ]
  },
  {
    "objectID": "tdv_session/01_TrainingDataCollection.html#dimension-reduction-pca",
    "href": "tdv_session/01_TrainingDataCollection.html#dimension-reduction-pca",
    "title": "Collection of training data for remote sensing model building",
    "section": "Dimension reduction (PCA)",
    "text": "Dimension reduction (PCA)\nIn a fist step we reduce the dimensions of the 9 Sentinel-2 bands while maintaining most of the information, using a principal component analysis (PCA).\n\n#PCA\npca&lt;-RStoolbox::rasterPCA(s2,nSamples = 5000, spca=TRUE )\n\nggRGB(pca$map,1,2,3, stretch=\"lin\", q=0)\n\nWarning in matrix(z, nrow = nrow(rr), ncol = ncol(rr), byrow = TRUE):\nDatenlänge [168814] ist kein Teiler oder Vielfaches der Anzahl der Zeilen [684]\n\n\n\n\n\n\n\n\n\nFrom the output of the PCA we see that we can capture 92% of the variability with the first two components. Thus we will only use the PC1 and PC2 for the subsequent analysis.",
    "crumbs": [
      "Training Data",
      "Collection of training data for remote sensing model building"
    ]
  },
  {
    "objectID": "tdv_session/01_TrainingDataCollection.html#unsupervised-clustering",
    "href": "tdv_session/01_TrainingDataCollection.html#unsupervised-clustering",
    "title": "Collection of training data for remote sensing model building",
    "section": "Unsupervised clustering",
    "text": "Unsupervised clustering\nIn the next step we run an unsupervised classification of the PC1 and PC2 to get a clustered map. For the unsupervised classification we need to take a decision on the number of classes/clusters to be created. Here we will take n=5 classes. However, depending on the target variable this value need to be adjusted.\n\nset.seed(2222)\ncluster &lt;- unsuperClass(pca$map[[c('PC1','PC2')]], nSamples = 100, nClasses = 5, nStarts = 5)\n\n\n## Plots\ncolors &lt;- rainbow(5)\nplot(cluster$map, col = colors, legend = TRUE, axes = TRUE, box =TRUE)\n\n\n\n\n\n\n\n\nThe map shows a clear spatial patterns related to the elevation, tree species and vitality status of the Nationalpark forests.",
    "crumbs": [
      "Training Data",
      "Collection of training data for remote sensing model building"
    ]
  },
  {
    "objectID": "tdv_session/01_TrainingDataCollection.html#implement-a-plot-design",
    "href": "tdv_session/01_TrainingDataCollection.html#implement-a-plot-design",
    "title": "Collection of training data for remote sensing model building",
    "section": "Implement a plot design",
    "text": "Implement a plot design\n\n# Create a training data set by extracting the mean value of all pixels touching\n# a buffered area with 13m around the plot center\ntrain&lt;-raster::extract(s2,samples,sp=T,buffer=13,fun='mean')\nmapview(train, zcol=\"class_unsupervised\",\n        map.types = c(\"Esri.WorldShadedRelief\", \"OpenStreetMap.DE\"))+\n  mapview(np_boundary,alpha.regions = 0.2, aplha = 1)",
    "crumbs": [
      "Training Data",
      "Collection of training data for remote sensing model building"
    ]
  },
  {
    "objectID": "templates/slides-template.html#section",
    "href": "templates/slides-template.html#section",
    "title": "YOUR TITLE",
    "section": "",
    "text": "This is a slide with a background image"
  },
  {
    "objectID": "templates/slides-template.html#bullets",
    "href": "templates/slides-template.html#bullets",
    "title": "YOUR TITLE",
    "section": "Bullets",
    "text": "Bullets\nRemove the incremental ::: bracketed div for plain lists\n\n\nthe quick\nbrown fox\njumps over\nthe lazy dog"
  },
  {
    "objectID": "templates/slides-template.html#columns",
    "href": "templates/slides-template.html#columns",
    "title": "YOUR TITLE",
    "section": "Columns",
    "text": "Columns\n\n\nSome text on the left of the slide\n\nSome text on the right of the slide"
  },
  {
    "objectID": "templates/slides-template.html#smaller-slide",
    "href": "templates/slides-template.html#smaller-slide",
    "title": "YOUR TITLE",
    "section": "Smaller Slide",
    "text": "Smaller Slide\nThe text on this slide will be, um, smaller."
  },
  {
    "objectID": "templates/slides-template.html#sneaky-info-asides-bootnotes",
    "href": "templates/slides-template.html#sneaky-info-asides-bootnotes",
    "title": "YOUR TITLE",
    "section": "Sneaky Info (Asides & Bootnotes)",
    "text": "Sneaky Info (Asides & Bootnotes)\nThis is cool! 1\nAdd reference-location: document to the YAML for end notes.\n\n\nI am at the bottom of the slide\nNo it is not"
  },
  {
    "objectID": "templates/slides-template.html#scrolly-slide",
    "href": "templates/slides-template.html#scrolly-slide",
    "title": "YOUR TITLE",
    "section": "Scrolly Slide",
    "text": "Scrolly Slide\nOverflowed content will be scrollable on this slide."
  },
  {
    "objectID": "templates/slides-template.html#bootnotes",
    "href": "templates/slides-template.html#bootnotes",
    "title": "YOUR TITLE",
    "section": "Bootnotes",
    "text": "Bootnotes"
  },
  {
    "objectID": "templates/slides-template.html#a-slide-with-a-plot",
    "href": "templates/slides-template.html#a-slide-with-a-plot",
    "title": "YOUR TITLE",
    "section": "A slide with a plot",
    "text": "A slide with a plot\n\n\n# ^^ could be fragment, slide, column, column-fragment\nggplot() +\n  geom_point(\n    data = mtcars,\n    aes(wt, mpg),\n    color = \"goldenrod\"\n  ) +\n  labs(\n    title = \"Some Dots\"\n  ) \n\n\n\n\n\n\n\n\n\n\n\n\nYOUR ORG\n\n\n\n\n\n\n\n \n\n\nthe incredible talk about making sense scales"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome EON Summer School 2024 (01.9-06.09-2024)",
    "section": "",
    "text": "Polsterberger Hubhaus 2022\n\n\n\n\n\n\n\nPolsterberger Hubhaus 2023",
    "crumbs": [
      "Welcome",
      "Welcome EON Summer School 2024 (01.9-06.09-2024)"
    ]
  },
  {
    "objectID": "index.html#sensors-equipment",
    "href": "index.html#sensors-equipment",
    "title": "Welcome EON Summer School 2024 (01.9-06.09-2024)",
    "section": "Sensors & Equipment",
    "text": "Sensors & Equipment\n\nHAWK:\n\nMavic 3 (Enterprise + Thermal + Multispectral)\nGNSS (Emlid, Alberding, Garmin)\nTablets (Android)\nForest Measurement Devices (diameter tapes, calipers, vertex, laser range finders, …)\nGeoSlam Mobile Laser Scanner\n\nUni Münster:\n\nMica Sens red edge (Multispektral)\nWIRIS thermal camera\nL1 Lidar (Dji)\nSony Alpha (42mp) RGB camera\nppm 10xx GNSS Sensor\n\nUni Marburg:\n\nMavic 3 (Enterprise + Thermal + Multispectral)\nMavic 3 Mini Pro\nLoRa based real time climate sensors",
    "crumbs": [
      "Welcome",
      "Welcome EON Summer School 2024 (01.9-06.09-2024)"
    ]
  },
  {
    "objectID": "slides/slide1.html#header-12",
    "href": "slides/slide1.html#header-12",
    "title": "Slides and extensions",
    "section": "Header (1|2)",
    "text": "Header (1|2)\nThe support of header and footer logic is provided by the plugin reveal-header. it is activated by:\nfilters:\n  - reveal-header"
  },
  {
    "objectID": "slides/slide1.html#header-22",
    "href": "slides/slide1.html#header-22",
    "title": "Slides and extensions",
    "section": "Header (2|2)",
    "text": "Header (2|2)\nIn this example you will find a basic header and footer text, pagination and a logo in the upper left corner .\n---\ntitle: \"Slides and extensions\"\nsubtitle: \"basically shows the 3 extensions samples\"\ntitle-slide-attributes:\n  data-background-image: slide1/mof.png\n  data-background-size: contain\n  data-background-opacity: \"0.5\"\nformat: \n  revealjs:\n    slide-number: true\n    footer: &lt;gisma 2023&gt;\n    header: This is the header extension\n    header-logo: slide1/logooil.jpg\n[...]\n---"
  },
  {
    "objectID": "slides/slide1.html#spotlight-12",
    "href": "slides/slide1.html#spotlight-12",
    "title": "Slides and extensions",
    "section": "Spotlight (1|2)",
    "text": "Spotlight (1|2)\nThe support of a pointer or similar pointing features is provided by the plugin spotlight. it is activated by:\nrevealjs-plugins:\n  - spotlight"
  },
  {
    "objectID": "slides/slide1.html#spotlight-22",
    "href": "slides/slide1.html#spotlight-22",
    "title": "Slides and extensions",
    "section": "Spotlight (2|2)",
    "text": "Spotlight (2|2)\nCurrently the spotlight is set to a red dot pointer. Just press the left mouse button and use it. It is defined in the header:\n---\n[...]\nformat: \n  revealjs:\n    slide-number: true\n    footer: &lt;gisma 2023&gt;\n    header: This is the header extension\n    header-logo: slide1/logooil.jpg\n    spotlight:\n      useAsPointer: true\n      size: 5\n\nfilters:\n  - roughnotation\n  - reveal-header\nrevealjs-plugins:\n  - spotlight\n---"
  },
  {
    "objectID": "slides/slide1.html#highlighting-concept",
    "href": "slides/slide1.html#highlighting-concept",
    "title": "Slides and extensions",
    "section": "Highlighting concept",
    "text": "Highlighting concept\nThe support of complex highlighting etc. is provided by the plugin roughnotation. it is activated by:\nfilters:\n  - roughnotation\nTo activate the highlighting interactively press the r key. It will start any notation animations:\nI will be highlighted, and so will these words right here"
  },
  {
    "objectID": "slides/slide1.html#options",
    "href": "slides/slide1.html#options",
    "title": "Slides and extensions",
    "section": "Options",
    "text": "Options\nThere are many types of options we can use (Press r to show)\n\ntype\nanimate\nanimationDuration\ncolor\nstrokeWidth\nmultiline multiline multiline multiline multiline multiline multiline multiline multiline multiline\niterations\nrtl"
  },
  {
    "objectID": "slides/slide1.html#options-1",
    "href": "slides/slide1.html#options-1",
    "title": "Slides and extensions",
    "section": "Options",
    "text": "Options\n(Press r to show)\nThe options are applied by adding arguments like so {.rn rn-color=orange rn-type=circle}\nSo to add a orange circle or turn off animations by adding rn-animate=false\nNote that the arguments are all prefixed with rn-, are not comma-separated, logical values are written as true or false and that strings do not have to be in quotes"
  },
  {
    "objectID": "slides/slide1.html#options---types",
    "href": "slides/slide1.html#options---types",
    "title": "Slides and extensions",
    "section": "Options - types",
    "text": "Options - types\n(Press r to show)\n\n\nUnderline\nBox\nCircle\nHighlight\nStrike-Through\nCrossed-off\n\nMany types to choose from!\nHyphenated options can be used like so rn-type=strike-through"
  },
  {
    "objectID": "slides/slide1.html#options---multiline",
    "href": "slides/slide1.html#options---multiline",
    "title": "Slides and extensions",
    "section": "Options - Multiline",
    "text": "Options - Multiline\n(Press r to show)\nThe options rn-multiline=true can be added to make a highligher work across multiple lines.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed accumsan nisi hendrerit augue molestie tempus. Phasellus purus quam, aliquet nec commodo quis, pharetra ut orci. Donec laoreet ligula nisl, placerat molestie mauris luctus id. Fusce dapibus non libero nec lobortis."
  },
  {
    "objectID": "slides/slide1.html#all-about-time",
    "href": "slides/slide1.html#all-about-time",
    "title": "Slides and extensions",
    "section": "All about Time",
    "text": "All about Time\n(Press r to show)\nUnless otherwise specified, all annotations will occur at the same time. Set the rn-index to specify order\nNo rn-index\nrn-index set to 1\nrn-index set to 2\nrn-index set to 3\nrn-index set to 4"
  },
  {
    "objectID": "slides/slide1.html#fenced-divs",
    "href": "slides/slide1.html#fenced-divs",
    "title": "Slides and extensions",
    "section": "Fenced divs",
    "text": "Fenced divs\nYou can also use fenced divs if you want to apply the changes to larger sections of of the slide\n::: {.rn rn-type=box rn-color=red}\nHere is some text\n\nAnd there is more here\n:::\n\nHere is some text\nAnd there is more here"
  },
  {
    "objectID": "slides/slide1.html#known-issues",
    "href": "slides/slide1.html#known-issues",
    "title": "Slides and extensions",
    "section": "Known issues",
    "text": "Known issues\ndoesn’t show correctly in RStudio IDE\nDepending on Browser and setting use the CTRL +/- zoom to place the highlights at the correct places"
  },
  {
    "objectID": "slides/slide1.html#basic-reference",
    "href": "slides/slide1.html#basic-reference",
    "title": "Slides and extensions",
    "section": "Basic Reference",
    "text": "Basic Reference\nFind more informations at Quarto RevealJS Documentation\n\n\n\n&lt;gisma 2023&gt;\n\n\n\n\n\n\n\n \n\n\nThis is the header extension"
  },
  {
    "objectID": "modules/en-git-module.html#module-overview",
    "href": "modules/en-git-module.html#module-overview",
    "title": "Git, GitHub & Rstudio [EN]",
    "section": "Module Overview",
    "text": "Module Overview\nThis module is about the version control system Git, the cloud services GitHub/GitLab and using them with RStudio as an IDE.\n\nContent"
  },
  {
    "objectID": "mc_session/mc1.html",
    "href": "mc_session/mc1.html",
    "title": "Microclimate Sensors & Power supply Units",
    "section": "",
    "text": "froggit shop [DE]\n  \n  \n    \n     ecowitt shop [US]\n  \n  \n    \n     Fine Offset\n  \n\n      \nThe sensors from Fine Offset are re-branded and partly modified by the resellers. This article deals with sensors from the german re-seller froggit and the US re-seller ecowitt. More precise the DP-/GW SmartHubs WiFi Gateway with temperature, humidity & Pressure which is developed by fine offset. The unique selling point of the LoRa-Wifi gateway is the extraordinarily extensive possibility of connecting radio-bound sensors.",
    "crumbs": [
      "Microclimate Data Retrieval & Spatial Data Interpolation",
      "Microclimate Sensors & Power supply Units"
    ]
  },
  {
    "objectID": "mc_session/mc1.html#calibration-concept",
    "href": "mc_session/mc1.html#calibration-concept",
    "title": "Microclimate Sensors & Power supply Units",
    "section": "Calibration Concept",
    "text": "Calibration Concept\nThe low budget sensors are usually lacking of a stable measurement quality. To obtain reliable micro climate data a two step calibration process is suggested.\n\nThe measurements of all sensors (preferably in a climate chamber) will be statistically analysed to identify sensor which produce systematic and significant outliers.\nThe sensors are calibrated against an operational running high price reference station in the field.\n\n\n\n\n\n\n\nFuture Calibration Plans\n\n\n\n\n\nFor the future a machine learning approach including the radiation, azimuth, temperature and humidity as predictors for the calibrated temperature as the response variable will be used as an rolling calibration tool.",
    "crumbs": [
      "Microclimate Data Retrieval & Spatial Data Interpolation",
      "Microclimate Sensors & Power supply Units"
    ]
  },
  {
    "objectID": "mc_session/mc1.html#switching-scheme",
    "href": "mc_session/mc1.html#switching-scheme",
    "title": "Microclimate Sensors & Power supply Units",
    "section": "Switching scheme",
    "text": "Switching scheme\nThe battery box has a very simple design. Besides the cabling, it contains a solar charge regulator, a fuse panel for the protection of the consumers and an AGM 120aH battery.\n ## Components * Sealable, durable Wham Bam Heavy Duty Box, 62 L, 59,5 x 40 x 37 cm, PP Recycling Plastic Wham Bam Box. The “Wham Bam Box” made of recycled PP plastic was chosen for its extreme mechanical strength and almost complete biochemical resistance. The bad temperature spectrum for thermal stability is from approx. -10 -140 °C., it is acid and alkali resistant and waterproof. By additionally equipping the box with a fire protection mat, the almost airtight closure offers a virtually complete reduction of fire load inside and outside the box. * 12V deep-cycle battery BSA Audio Solar Technologie 120 Ah 12V C100 * 3 x Neutrik powerCON TRUE1 NAC3FPX outlets and Neutrik SCNAC-FPX sealing cover. * Fuse Box for car fuses up to max. 15A per fuse, maximum 30A per fuse box, With sealed cover, splash-proof, Material: PA6.6, 12 connections on the side * Nominal voltage: 32 V/DC * Nominal current (per output): 15 A * Temperature range: -20 - +85 °C * Connections: Flat plug 8x 6,3 x 0,8 mm lateral * Solar charge controller, 20A (ALLPOWERS, available from various brands) Specification ALLPOWERS",
    "crumbs": [
      "Microclimate Data Retrieval & Spatial Data Interpolation",
      "Microclimate Sensors & Power supply Units"
    ]
  },
  {
    "objectID": "mc_session/mc1.html#wiring",
    "href": "mc_session/mc1.html#wiring",
    "title": "Microclimate Sensors & Power supply Units",
    "section": "Wiring",
    "text": "Wiring\n\nBattery to solar charger:\n\nPole terminal connectors (+ and -)\n6 mm2 cables (red and black)\n2 x Crimp cable shoes\n\nSolar panel to solar charger\n\nMC4 photovoltaic connectors (+ and -) Weidemüller\n6 mm2 cables (red and black)\n2x Crimp cable shoes\n\nSolar charger fuse box outlets\n\n6 x 1,5 mm2 cables, red\n6 x 1/4’’ FASTON terminals Fuse Box\n3 x 1,5 mm2 cables, black\n2 x Crimp cable shoes (holding 3 wires)\n6 x 6,35mm / 1/4’’ crimp FASTON terminals\n\n\nPlease note the following points: * Silicone cables, solar cables, plugs and fuse box fulfills industry standards. All cable lugs are crimped and checked. * The cable lugs are not screwed to the charging cables with cable lugs but through the crimp connection with the end sleeve. * A main fuse (e.g. 40A automatic circuit breaker) must be installed\nSee also the figure below.",
    "crumbs": [
      "Microclimate Data Retrieval & Spatial Data Interpolation",
      "Microclimate Sensors & Power supply Units"
    ]
  },
  {
    "objectID": "mc_session/mc1.html#mounting",
    "href": "mc_session/mc1.html#mounting",
    "title": "Microclimate Sensors & Power supply Units",
    "section": "Mounting",
    "text": "Mounting\n\nOutlets: 6x M3 screw (12mm), washers and nuts\nSolar connectors: 2 x waterproof cable glands\nSolar charger and fuse box:\n\nWooden plate, glued to the box\n4 screws for Solar Charge Controller\n4 screws for fuse box Cable lugs and plugs are covered with self-vulcanizing tape and additionally insulated.",
    "crumbs": [
      "Microclimate Data Retrieval & Spatial Data Interpolation",
      "Microclimate Sensors & Power supply Units"
    ]
  },
  {
    "objectID": "mc_session/mc1.html#station-setup-in-the-field",
    "href": "mc_session/mc1.html#station-setup-in-the-field",
    "title": "Microclimate Sensors & Power supply Units",
    "section": "Station setup in the field",
    "text": "Station setup in the field\nFor safe operation, the following points must be taken into account when setting up the box:\n1.) The box must be placed horizontally. Preferable at on a clearing to reduce impacts of falling branches or similar.  2.) One square meter around the box must be cleared of any vegetation and the A-horizon (depending on the slope, even more).\n 3.) Around this area a further strip with a diameter of at least 1 meter must also be cleared of organic material, especially leaves. Dig up the A-horizon and exclude roots and organic stuff. Note that the wiring sections must also be cleared of combustible organic material.\n 4.) Check cables and screws for proper seating and integrity.\n\n5.) Check proper installation of the solar panel. Mount the panel on a simple wooden slat attached to the frame to avoid damage to the protective foil on the back. Such damage will destroy the panel.\n\n6.) Attach the solar connectors to the panel. This avoids ground contact and provides good weather protection. This can be done very easily by threading cable ties through the plugs and the junction box. {% include figure image_path=“../images/battery_box/07_solar_plugs.jpg” alt=“Attach the solar connectors to the panel.” %}  7.) Finally, the box should be secured against unauthorized or accidental opening. For this purpose there is a steel cable with a number lock, which is to be attached in the way it is placed there.",
    "crumbs": [
      "Microclimate Data Retrieval & Spatial Data Interpolation",
      "Microclimate Sensors & Power supply Units"
    ]
  },
  {
    "objectID": "mc_session/mc1.html#final-check",
    "href": "mc_session/mc1.html#final-check",
    "title": "Microclimate Sensors & Power supply Units",
    "section": "Final check",
    "text": "Final check\n\nAll contacts and cables must be checked for proper seating and integrity. Especially the charging cables on the battery must be screwed tightly.\nAll cables are to be laid without tension.\nThe solar cables are to be laid separately to avoid a short circuit, so that an animal crossing etc. does not cause them to come into contact.\nThe box is secured and tight.",
    "crumbs": [
      "Microclimate Data Retrieval & Spatial Data Interpolation",
      "Microclimate Sensors & Power supply Units"
    ]
  },
  {
    "objectID": "mc_session/mc1.html#risk-assessment",
    "href": "mc_session/mc1.html#risk-assessment",
    "title": "Microclimate Sensors & Power supply Units",
    "section": "Risk Assessment",
    "text": "Risk Assessment\nHere you find the preliminary risk assesment for the installation and operation of 12 V solar power based energy supply units and measuring sensor systems.",
    "crumbs": [
      "Microclimate Data Retrieval & Spatial Data Interpolation",
      "Microclimate Sensors & Power supply Units"
    ]
  },
  {
    "objectID": "base/faq.html",
    "href": "base/faq.html",
    "title": "Frequently asked Questions",
    "section": "",
    "text": "This is a senseless question to meet a meaningfull answer\n\n\n\n\n\n\n\n\n\nThis is a meaningful answer to a senseless question\n\n\n\n\n\n\n\n\n\nLearn More…\n\n\n\n\n\nThis is a even more meaningful answer to a senseless question"
  },
  {
    "objectID": "base/faq.html#make-sense-topic",
    "href": "base/faq.html#make-sense-topic",
    "title": "Frequently asked Questions",
    "section": "",
    "text": "This is a senseless question to meet a meaningfull answer\n\n\n\n\n\n\n\n\n\nThis is a meaningful answer to a senseless question\n\n\n\n\n\n\n\n\n\nLearn More…\n\n\n\n\n\nThis is a even more meaningful answer to a senseless question"
  },
  {
    "objectID": "base/about.html",
    "href": "base/about.html",
    "title": "About this site",
    "section": "",
    "text": "About this site\nThis page summarizes the essential workflows , basic literature and web resources from the distributed course systems , documents and field protocols into a knowledge base.\nAlthough the web space is topic-centered any keyword can be searched using the full text search.\nThe creation of new pages, the editing of existing pages can be triggered directly via the right column online.\nOffline there are several visual editors and full integration with Rstudio etc."
  },
  {
    "objectID": "base/impressum.html#content-responsibility",
    "href": "base/impressum.html#content-responsibility",
    "title": "Impressum",
    "section": "Content Responsibility",
    "text": "Content Responsibility\nThe responsibility for the content rests with the instructors. Statements, opinions and/or conclusions are the ones from the instructors and do not necessarily reflect the opinion of the representatives of Marburg University."
  },
  {
    "objectID": "base/impressum.html#content-license",
    "href": "base/impressum.html#content-license",
    "title": "Impressum",
    "section": "Content License",
    "text": "Content License\nThis work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.\n\nPrivacy Policy\n\n\nAs of 21. October 2021\n\n\nIntroduction\n\n\nWith the following data protection declaration, we would like to inform you about the types of your personal data (hereinafter also referred to as “data” for short) that we process, for what purposes and to what extent. The privacy policy applies to all processing of personal data carried out by us, both in the context of the provision of our services and in particular on our websites, in mobile applications and within external online presences, such as our social media profiles (hereinafter collectively referred to as “Online Offerings”).\n\n\nThe terms used are not gender-specific.\n\n\nResponsible\n\n\nDr Christoph ReudenbachDeutschhaustr 1035037 Marburg\n\n\nEmail address: reudenbach@uni-marburg.de.\n\n\nImprint: https://www.uni-marburg.de/de/impressum.\n\n\nOverview of Processing\n\n\nThe following overview summarizes the types of data processed and the purposes of their processing, and refers to the data subjects.\n\n\nTypes of Data Processed\n\n\n\nContent data (e.g. input in online forms).\n\n\nContact data (e.g. email, phone numbers).\n\n\nMeta/communication data (e.g. device information, IP addresses).\n\n\nUse data (e.g. websites visited, interest in content, access times).\n\n\n\nCategories of data subjects\n\n\n\nCommunication partners.\n\n\nUsers (e.g.. Website visitors, users of online services).\n\n\n\nPurposes of processing\n\n\n\nDirect marketing (e.g., by email or postal mail).\n\n\nContact requests and communications.\n\n\n\nRelevant legal basis\n\n\nThe following is an overview of the legal basis of the GDPR on the basis of which we process personal data. Please note that in addition to the provisions of the GDPR, national data protection regulations may apply in your or our country of residence or domicile. Furthermore, should more specific legal bases be decisive in individual cases, we will inform you of these in the data protection declaration.\n\n \n\n\nConsent (Art. 6 para. 1 p. 1 lit. a. DSGVO) - The data subject has given his or her consent to the processing of personal data concerning him or her for a specific purpose or purposes.\n\n\nRegistered interests (Art. 6 para. 1 p. 1 lit. f. DSGVO) - Processing is necessary to protect the legitimate interests of the controller or a third party, unless such interests are overridden by the interests or fundamental rights and freedoms of the data subject which require the protection of personal data.\n\n\n\nNational data protection regulations in Germany: In addition to the data protection regulations of the General Data Protection Regulation, national regulations on data protection apply in Germany. These include, in particular, the Act on Protection against Misuse of Personal Data in Data Processing (Federal Data Protection Act - BDSG). In particular, the BDSG contains special regulations on the right to information, the right to erasure, the right to object, the processing of special categories of personal data, processing for other purposes and transmission, as well as automated decision-making in individual cases, including profiling. Furthermore, it regulates data processing for employment purposes (Section 26 BDSG), in particular with regard to the establishment, implementation or termination of employment relationships as well as the consent of employees. Furthermore, state data protection laws of the individual federal states may apply.\n\n \n\nSecurity measures\n\n\nWe take appropriate technical and organizational measures in accordance with the legal requirements, taking into account the state of the art, the implementation costs and the nature, scope, circumstances and purposes of the processing, as well as the different probabilities of occurrence and the extent of the threat to the rights and freedoms of natural persons, in order to ensure a level of protection appropriate to the risk.\n\n.\n\nMeasures include, in particular, ensuring the confidentiality, integrity, and availability of data by controlling physical and electronic access to data as well as access to, entry into, disclosure of, assurance of availability of, and segregation of data concerning them. Furthermore, we have established procedures to ensure the exercise of data subjects’ rights, the deletion of data, and responses to data compromise. Furthermore, we take the protection of personal data into account as early as the development or selection of hardware, software as well as procedures in accordance with the principle of data protection, through technology design and through data protection-friendly default settings.\n\n \n\nDeletion of data\n\n\nThe data processed by us will be deleted in accordance with legal requirements as soon as their consents permitted for processing are revoked or other permissions cease to apply (e.g. if the purpose of processing this data has ceased to apply or it is not necessary for the purpose).\n\n \n\nIf the data are not deleted because they are required for other and legally permissible purposes, their processing will be limited to these purposes. That is, the data will be blocked and not processed for other purposes. This applies, for example, to data that must be retained for reasons of commercial or tax law or whose storage is necessary for the assertion, exercise or defense of legal claims or for the protection of the rights of another natural person or legal entity.\n\n \n\nOur privacy notices may also include further information on the retention and deletion of data that takes precedence for the processing operations in question.\n\n \n\nUse of cookies\n\n\nCookies are text files that contain data from websites or domains visited and are stored by a browser on the user’s computer. The primary purpose of a cookie is to store information about a user during or after their visit within an online site. Stored information may include, for example, language settings on a website, login status, a shopping cart, or where a video was watched. We further include in the term cookies other technologies that perform the same functions as cookies (e.g., when user details are stored using pseudonymous online identifiers, also referred to as “user IDs”)\n\n.\n\nThe following cookie types and functions are distinguished:\n\n\n\nTemporary cookies (also: session or session cookies): Temporary cookies are deleted at the latest after a user has left an online offer and closed his browser.\n\n\nPermanent cookies: Permanent cookies remain stored even after closing the browser. For example, the login status can be saved or preferred content can be displayed directly when the user revisits a website. Likewise, the interests of users used for range measurement or marketing purposes can be stored in such a cookie.\n\n\nFirst-party cookies: First-party cookies are set by ourselves.\n\n\nThird-party cookies (also: third-party cookies): Third-party cookies are mainly used by advertisers (so-called third parties) to process user information.\n\n\nNecessary (also: essential or absolutely necessary) cookies: Cookies may be absolutely necessary for the operation of a website (e.g. to store logins or other user input or for security reasons).\n\n\nStatistics, marketing and personalization cookies: Furthermore, cookies are usually also used in the context of range measurement and when the interests of a user or his behavior (e.g. viewing certain content, use of functions, etc.) on individual web pages are stored in a user profile. Such profiles are used, for example, to show users content that matches their potential interests. This process is also referred to as “tracking”, i.e., tracking the potential interests of users. Insofar as we use cookies or “tracking” technologies, we will inform you separately in our privacy policy or in the context of obtaining consent.\n\n\n\nNotes on legal bases: On which legal basis we process your personal data using cookies depends on whether we ask you for consent. If this is the case and you consent to the use of cookies, the legal basis for the processing of your data is the declared consent. Otherwise, the data processed with the help of cookies is processed on the basis of our legitimate interests (e.g. in a business operation of our online offer and its improvement) or, if the use of cookies is necessary to fulfill our contractual obligations.\n\n.\n\nDuration of storage: If we do not provide you with explicit information about the storage period of permanent cookies (e.g. in the context of a so-called cookie opt-in), please assume that the storage period can be up to two years.\n\n.\n\nGeneral information on revocation and objection (opt-out):  Depending on whether the processing is based on consent or legal permission, you have the option at any time to revoke any consent given or to object to the processing of your data by cookie technologies (collectively referred to as “opt-out”). You can initially declare your objection by means of your browser settings, e.g. by deactivating the use of cookies (whereby this may also restrict the functionality of our online offer). An objection to the use of cookies for online marketing purposes can also be declared by means of a variety of services, especially in the case of tracking, via the websites https://optout.aboutads.info and https://www.youronlinechoices.com/. In addition, you can receive further objection notices in the context of the information on the service providers and cookies used.\n\n.\n\nProcessing of cookie data on the basis of consent: We use a cookie consent management procedure, in the context of which the consent of users to the use of cookies, or the processing and providers mentioned in the cookie consent management procedure can be obtained and managed and revoked by users. Here, the declaration of consent is stored in order not to have to repeat its query and to be able to prove the consent in accordance with the legal obligation. The storage can take place on the server side and/or in a cookie (so-called opt-in cookie, or with the help of comparable technologies), in order to be able to assign the consent to a user or their device. Subject to individual information on the providers of cookie management services, the following information applies: The duration of the storage of consent can be up to two years. Here, a pseudonymous user identifier is formed and stored with the time of consent, information on the scope of consent (e.g., which categories of cookies and/or service providers) as well as the browser, system and end device used.\n\n.\n\n\nTypes of data processed: Usage data (e.g. websites visited, interest in content, access times), meta/communication data (e.g. device information, IP addresses).\n\n\nPersons concerned: Users (e.g. website visitors, users of online services).\n\n\nLegal basis: Consent (Art. 6 para. 1 p. 1 lit. a. DSGVO), Legitimate Interests (Art. 6 para. 1 p. 1 lit. f. DSGVO).\n\n\n\nSurveys and polls\n\n\nThe surveys and polls (hereinafter “surveys”) conducted by us are evaluated anonymously. Personal data is only processed insofar as this is necessary for the provision and technical implementation of the surveys (e.g. processing of the IP address to display the survey in the user’s browser or to enable a resumption of the survey with the help of a temporary cookie (session cookie)) or users have consented.\n\n.\n\nNotes on legal basis: If we ask participants for consent to process their data, this is the legal basis of the processing, otherwise the processing of participants’ data is based on our legitimate interests in conducting an objective survey.\n\n \n\n\nTypes of data processed: Contact data (e.g. email, phone numbers), content data (e.g. input in online forms), usage data (e.g. web pages visited, interest in content, access times), meta/communication data (e.g. device information, IP addresses).\n\n\nParticipants concerned: Communication partners.\n\n\nPurposes of processing: Contact requests and communication, direct marketing (e.g. by e-mail or postal mail).\n\n\nLegal basis: Consent (Art. 6 para. 1 p. 1 lit. a. DSGVO), Legitimate Interests (Art. 6 para. 1 p. 1 lit. f. DSGVO).\n\n\n\nChange and Update Privacy Policy\n\n\nWe encourage you to periodically review the contents of our Privacy Policy. We adapt the Privacy Policy as soon as the changes in the data processing activities we carry out make it necessary. We will inform you as soon as the changes require an act of cooperation on your part (e.g. consent) or other individual notification.\n\n.\n\nWhere we provide addresses and contact information for companies and organizations in this Privacy Policy, please note that addresses may change over time and please check the information before contacting us.\n\n.\n\nRights of data subjects\n\n\nAs a data subject, you are entitled to various rights under the GDPR, which arise in particular from Art. 15 to 21 DSGVO:\n\n\n\nRight to object: You have the right to object at any time, on grounds relating to your particular situation, to the processing of personal data relating to you which is carried out on the basis of Art. 6(1)(e) or (f) DSGVO; this also applies to profiling based on these provisions. If the personal data concerning you is processed for the purpose of direct marketing, you have the right to object at any time to the processing of personal data concerning you for the purpose of such marketing; this also applies to profiling, insofar as it is associated with such direct marketing.\n\n\nRight of withdrawal in the case of consent: You have the right to withdraw any consent you have given at any time.\n\n\nRight of access: You have the right to request confirmation as to whether data in question is being processed and to information about this data, as well as further information and copy of the data in accordance with the legal requirements.\n\n\nRight of rectification: You have the right, in accordance with the legal requirements, to request the completion of the data concerning you or the correction of incorrect data concerning you.\n\n\nRight to erasure and restriction of processing: You have, in accordance with the law, the right to request that data concerning you be erased without undue delay, or alternatively, in accordance with the law, to request restriction of the processing of the data.\n\n\nRight to data portability: You have the right to receive data concerning you, which you have provided to us, in a structured, common and machine-readable format in accordance with the legal requirements, or to demand its transfer to another responsible party.\n\n\nComplaint to supervisory authority: Without prejudice to any other administrative or judicial remedy, you have the right to lodge a complaint with a supervisory authority, in particular in the Member State of your habitual residence, place of work or the place of the alleged infringement, if you consider that the processing of personal data concerning you infringes the requirements of the GDPR.\n\n\n.\n\nDefinitions of Terms\n\n\nThis section provides you with an overview of the terms used in this Privacy Policy. Many of the terms are taken from the law and defined primarily in Article 4 of the GDPR. The legal definitions are binding. The following explanations, on the other hand, are primarily intended to aid understanding. The terms are sorted alphabetically.\n\n \n\n\nPersonal data: “Personal data” means any information relating to an identified or identifiable natural person (hereinafter “data subject”); an identifiable natural person is one who can be identified, directly or indirectly, in particular by reference to an identifier such as a name, an identification number, location data, an online identifier (eg. e.g. cookie) or to one or more special characteristics that are an expression of the physical, physiological, genetic, mental, economic, cultural or social identity of that natural person.\n\n\nController: The “controller” is the natural or legal person, public authority, agency or other body which alone or jointly with others determines the purposes and means of the processing of personal data.\n\n\nProcessing: “Processing” means any operation or set of operations which is performed upon personal data, whether or not by automatic means. The term is broad and includes virtually any handling of data, whether collecting, evaluating, storing, transmitting or deleting.\n\n\n\nCreated with free Datenschutz-Generator.de by Dr. Thomas Schwenke"
  },
  {
    "objectID": "base/impressum.html#comments-suggestions",
    "href": "base/impressum.html#comments-suggestions",
    "title": "Impressum",
    "section": "Comments & Suggestions",
    "text": "Comments & Suggestions"
  },
  {
    "objectID": "mc_session/mc4.html",
    "href": "mc_session/mc4.html",
    "title": "Data Sources",
    "section": "",
    "text": "Station inside the dead Fir plot (Ecowitt Dashboard Login required)\n  \n  \n    \n     Station inside the clear cut (Ecowitt Dashboard Login required)\n  \n  \n    \n     Position Data of the climate stations\n  \n\n      \nData logger and measurement data",
    "crumbs": [
      "Microclimate Data Retrieval & Spatial Data Interpolation",
      "Data Sources"
    ]
  },
  {
    "objectID": "modules/slidelist.html",
    "href": "modules/slidelist.html",
    "title": "Self-study modules",
    "section": "",
    "text": "Git, GitHub & Rstudio [DE]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGit, GitHub & Rstudio [EN]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "slides/slidelist.html",
    "href": "slides/slidelist.html",
    "title": "Presentations",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nSlides and extensions\n\n\ngisma team\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "modules/de-git-module.html#modulüberblick",
    "href": "modules/de-git-module.html#modulüberblick",
    "title": "Git, GitHub & Rstudio [DE]",
    "section": "Modulüberblick",
    "text": "Modulüberblick\nIn diesem Modul geht es um die Versionskontrolle Git, die Cloud-Dienste GitHub/GitLab und deren Verwendung mit RStudio als IDE.\nGit ist ein Versionskontrollsystem, das es uns ermöglicht, Snapshots einer Datei oder sogar eines ganzen Projekts zu bestimmten Zeitpunkten zu erstellen. Außerdem bietet es eine komfortable Möglichkeit, diese Snapshots mit denen von Kollegen zu kombinieren.\nGitHub/GitLab bauen auf Git auf und sind die beiden beliebtesten Cloud-basierten Arbeitsumgebungen, die auf Git basieren und darüber hinaus eine breite Palette webbasierter Tools und Dienste anbieten.\nRStudio ist eine typische Desktop-Anwendung, eine sogenannte integrierte Entwicklungsumgebung (IDE), die nicht nur auf R/Python/JS und Markup Language basiert, sondern auch das Lesen/Schreiben, Manipulieren und Visualisieren von Daten und Texten ermöglicht und nahezu vollständige Unterstützung bei der Erstellung von Dokumenten in Form von Texten aller denkbaren Ausgabeformate, interaktiven Dokumenten und Websites bietet.\nNeben vielen kollaborativen Diensten und grundlegenden Versionskontrollfunktionen ist das Wichtigste, dass man eigentlich nichts kaputt machen kann.\n\nLernziele\nDie Lektionen dieses Moduls sind\n\nWas ist Versionskontrolle und GitHub?\nGit: Pull, Status, Add, Commit, Push\nVerzweigungen in GitHub\nUmgang mit Konflikten"
  },
  {
    "objectID": "modules/de-git-module.html#git-und-github-leicht-gemacht",
    "href": "modules/de-git-module.html#git-und-github-leicht-gemacht",
    "title": "Git, GitHub & Rstudio [DE]",
    "section": "Git und GitHub leicht gemacht",
    "text": "Git und GitHub leicht gemacht\n\nLernziele\nIn dieser Lektion lernst du\n\nVersionskontrolle verstehen\nGitHub und Git verstehen\n\n\n\nVorkenntnisse\n\nMit deinem spezifischen Dateimanager navigieren und arbeiten.\nOrdnerstrukturen verstehen\n\n\n\nÜberblick\nIn gängigen Office-Systemen können Sie z.B. AutoSave aktivieren, um kontinuierlich, zu bestimmten Zeiten oder manuell ein Backup zu erstellen. Die Versionskontrolle git funktioniert auf ähnliche Weise für von dir definierte Ordnerverzeichnisse.\nDu hast zum Beispiel einen Ordner, in dem du ein Projekt hast, das aus verschiedenen Dateien besteht (Text, Programmcode, Bilder, Sounddateien usw.) und du möchtest die Änderungen, die du an diesen Dateien gemacht hast, im Auge behalten.\nGit protokolliert alle Änderungen an diesen Dateien. 1. Git mitteilen, dass eine Datei oder ein Verzeichnis verfolgt werden soll. 2. dass der Zustand der Datei zu einem bestimmten Zeitpunkt aufgezeichnet werden soll.\nIm Gegensatz zum kontinuierlichen Backup von z.B. Google Docs (das keine Wiederherstellung erlaubt) ist dieser Prozess notwendigerweise in 2 Schritte aufgeteilt, um definierte Änderungen vornehmen zu können, bevor diese bestätigt und mit einem commit als Snapshot gespeichert werden.\n\nGit - Erste Schritte\nBei der Verwendung von Git muss zunächst ein Repository in einem Verzeichnis auf dem lokalen Rechner aktiviert werden. Dies geschieht mit dem Befehl git init. Nun weiß Git wo, aber nicht was es verfolgen soll.\n\nWährend der Arbeit muss Git “informiert” werden, was mit diesen Dateien geschehen soll. Dies geschieht mit den beiden Befehlen git add und git commit.\nEin wichtiger zusätzlicher Befehl, git push, wird verwendet, um den aktuellen Verzeichnis-Snapshot in ein entferntes Repository (z.B. Github, GitLab) zu übertragen.\n\nDer letzte Befehl ist git status. Du solltest diesen Befehl benutzen, wenn du an deinem Projekt arbeitest, damit du weißt, was du noch nicht getrackt hast. Die Ausgabe dieses Befehls besteht aus mehreren Teilen Du solltest in der Lage sein, die Ausgabe dieses Befehls zu interpretieren:\n\nWenn du mehr über Git erfahren möchtest, findest du hier weitere hilfreiche Ressourcen:\n\nPro Git: Kapitel Git Grundlagen\nHappy Git mit R"
  },
  {
    "objectID": "modules/de-git-module.html#gitgithub-pull-status-add-commit-push",
    "href": "modules/de-git-module.html#gitgithub-pull-status-add-commit-push",
    "title": "Git, GitHub & Rstudio [DE]",
    "section": "Git/GitHub: pull, status, add, commit, push",
    "text": "Git/GitHub: pull, status, add, commit, push\n\nLernziele\nIn dieser Lektion lernst du\n\nein lokales Projektarchiv in einem Ordner anlegst\nÄnderungen an einem entfernten Repository vornehmen\nein lokales Repository zu verwalten\n\n\n\nVoraussetzungen\n\nEinrichten eines GitHub-Accounts\nHerunterladen der Git Bash\n\n\n\nEinrichtung von git und GitHub\nEs gibt zwei typische Szenarien für die Einrichtung von Git und GitHub.\n\ndu hast das Projekt noch nicht gestartet und möchtest ein GitHub-Repository, das du als Vorlage auf deinen Rechner kopieren (klonen) und dann lokal mit Dateien und Verzeichnissen nach deinen Wünschen füllen kannst.\nDu hast das Projekt bereits lokal gestartet und möchtest es auf GitHub kopieren.\n\nBeide Szenarien werden von Jenny Bryan exzellent erläutert:\n\nSzenario 1: Happy Git With R: Kapitel 15 Neues Projekt GitHub\nSzenario 2: Happy Git With R: Kapitel 17 Bestehendes Projekt, GitHub\n\n\n\nSelbst-Check\n\n\n\n\n\nGut zu wissen\n\n\nDu versuchst, git commit auszuführen, nachdem du Änderungen an einer Datei vorgenommen hast, aber du trackst diese Datei(en) nicht. Deshalb müssen Sie zuerst git add ausführen.\nDu versuchst git push auszuführen, um Deine Aktualisierungen in das entfernte Repository zu übertragen, aber dieses existiert nicht.\nDu versuchst git push auszuführen, um deine Aktualisierungen in das entfernte Repository zu übertragen, obwohl es bereits neue Aktualisierungen im entfernten Repository gibt (z.B. von einem anderen Teammitglied), die du noch nicht in das lokale Projekt übertragen hast. Die Fehlermeldung, die du bekommst, wird in etwa so aussehen:\n\n\nFehler: Deine lokalen Änderungen an den folgenden Dateien würden beim Zusammenführen überschrieben: … Bitte übertrage oder speichere deine Änderungen vor dem Zusammenführen.\n\nDu weist also dein lokales git an, deine eigenen Änderungen hinzuzufügen, ohne die Änderungen deines Teamkollegen zu berücksichtigen - ein klassischer Loyalitätskonflikt. Der beste Weg, dieses Problem zu vermeiden, ist immer einen git pull durchzuführen, bevor man mit dem lokalen Editieren beginnt.\nFür ein besseres Verständnis lies die folgenden Texte:\n\nPull tricky.\nGit Grundlagen\nGit und R\nRstudio - git - GitHub"
  },
  {
    "objectID": "modules/de-git-module.html#fork-und-branches-auf-github",
    "href": "modules/de-git-module.html#fork-und-branches-auf-github",
    "title": "Git, GitHub & Rstudio [DE]",
    "section": "Fork und Branches auf GitHub",
    "text": "Fork und Branches auf GitHub\n\nLernziele\nIn dieser Lektion lernst du\n\nWas ein Fork/Branch eines GitHub-Repositorys ist.\nWie man einen Branch eines GitHub Repositories erstellt.\nWie Du ein GitHub Repository von einem Branch aus aktualisierst.\n\n\n\nVorausetzungen\n\nVertrautheit mit GitHub-Repositorys.\nGit muss auf deinem Computer installiert sein.\nEin GitHub Konto!\n\n\n\nWas ist ein Fork/Branch?\nWenn man in Gruppen an GitHub-Projekten arbeitet, wird es lästig, wenn eine Person den gesamten Code alleine in das Repository einpflegen muss. Hier kommen Forks und Branches ins Spiel. - Mit Branches kannst Du eine Kopie des aktuellen GitHub-Projekts nehmen und auf Deinem eigenen Computer Änderungen vornehmen. Sobald Du und Deine Gruppe Änderungen am Code vorgenommen habt, könnt Ihr die Änderungen wieder in Eure ursprüngliche Projektgruppe einfügen. - Branches können auch verwendet werden, wenn Du an einem Teil eines Projekts getrennt von den anderen Teilen arbeiten möchtest. - Forks sind sehr ähnlich, mit dem Unterschied, dass sie Kopien bzw Klone eines kompletten Projekts an einem anderen Ort sind.\n\nWie erstelle ich einen Branch?\nUm einen Branch von einem GitHub Repository zu erstellen, gehe zu dem Hauptrepository, an dem du arbeiten möchtest und klicke auf das Dropdown-Menü, das “main” heißen sollte. Es sollte wie das folgende Bild aussehen.\n\nSobald man auf dieses Menü klickt, erscheint auf GitHub ein Textfeld mit der Aufschrift “Find or create a branch…”, man gibt einen neuen Namen für den Zweig ein, z.B. ‘newbranch1’. Da dieser Zweig noch nicht existiert, fragt dich GitHub, ob du einen Zweig mit dem Namen “newbranch1” erstellen möchtest. Klicke auf “Create branch: newbranch1” und der neue Zweig wird für Dich erstellt, wie in der folgenden Abbildung zu sehen ist.\n\n\n\nWie stellt man einen Pull Request?\nEine Pull-Anfrage ermöglicht es dem Eigentümer des GitHub-Projekts, Deine Änderungen zu überprüfen, um sicherzustellen, dass sie in das aktuelle Repository passen und keine Konflikte in Deinem Repository verursachen.\nUm eine Pull-Anfrage von Deinem Zweig aus zu stellen, musst Du zuerst eine Änderung an Deinem Zweig-Repository vornehmen. Sobald Du eine Änderung an Deinem Zweig vorgenommen hast, erscheint ein gelber Balken auf Deinem Bildschirm, der Dich fragt, ob Du eine Pull-Anfrage stellen möchtest. Wie Du auf dem Bild unten sehen kannst, gibt es einen grünen Button, und sobald Du darauf klickst, kannst Du eine Pull-Anfrage erstellen.\n\nSobald Du auf den Button klickst, informiert Dich GitHub, ob es Probleme beim Zusammenführen des Zweigs mit dem Hauptprojekt gibt. Wenn es keine Probleme gibt, setzt GitHub ein Häkchen und zeigt “Able to merge” an. Du kannst dann einen Titel und einen Kommentar zu Deiner Pull-Anfrage hinzufügen, um den Besitzer des Repositorys darüber zu informieren, was Du getan hast. Sobald Du einen Kommentar und einen Titel eingegeben hast, kannst Du auf “Create a pull request” klicken. Wenn Du dies getan hast, wird eine Benachrichtigung an den Besitzer des Repositorys gesendet, dass Deine Änderungen zur Überprüfung bereit sind.\nNachdem Du Deine Anfrage abgeschickt hast, kann der Besitzer des GitHub-Projekts auf die Seite des Projekts gehen und auf den Reiter “Pull Requests” klicken. Auf dieser Seite wird eine Liste von Pull Requests angezeigt, aus der der Eigentümer Deine Anfrage auswählen kann. Sobald der Besitzer auf der Pull Request Seite angekommen ist, sieht er eine Schaltfläche mit der Aufschrift “Merge pull request” (ähnlich der Abbildung unten).\n\nSobald der Eigentümer auf die grüne Schaltfläche klickt, wird er erneut gefragt, ob er die Änderung vornehmen möchte. Wenn er erneut auf den Button klickt, wird die Änderung mit dem Hauptzweig zusammengeführt und er sieht etwas wie das folgende Bild…\n\n\n\n\nEin Repository in einem Branch (oder Fork) aktualisieren\nWenn jemand in deiner Gruppe eine Änderung am Master Repo vornimmt, gibt es eine Möglichkeit, deinen Zweig zu aktualisieren, damit du die Änderungen sehen kannst. Wenn eine Änderung vorgenommen wurde, wird auf der Webseite des verzweigten Repos angezeigt, dass Dein Repo “1 Commit behind the Master” ist. Das bedeutet, dass es 1 Änderung zwischen Deinem Fork und dem Main Repository gibt.\nWenn Du Deinen Fork aktualisieren möchtest, klicke auf die Schaltfläche “Änderungen”. Du wirst dann auf eine Seite geleitet, die sagt “main is up to date with all commits from branch. Versuchen Sie die Basis zu ändern”. Klicke auf “Change base”. Dann wird angezeigt, ob der Zweig zusammengeführt werden kann. Wenn ja, klicke auf “Create pull request” (Titel und Kommentar für deine Anfrage) und erstelle eine Pull-Anfrage.\nNun klicke auf Merge pull request, dann auf Confirm merge und dein Zweig wird aktualisiert!\n\n\nSelbst-Check\n\n\n\n\n\nGut zu wissen\n\n\nLerne wie man Branches mit dem Terminal erstellt: Arbeiten mit Branches\nLerne die Verwendung von Pull Requests und Issues: Issues und Pull Requests\nLerne, wie man ein GitHub-Repository forkt: Forken eines Repositories"
  },
  {
    "objectID": "modules/de-git-module.html#umgang-mit-konflikten",
    "href": "modules/de-git-module.html#umgang-mit-konflikten",
    "title": "Git, GitHub & Rstudio [DE]",
    "section": "Umgang mit Konflikten",
    "text": "Umgang mit Konflikten\n\nLernziele\nIn dieser Lektion lernst du\n\nWie man mit Konflikten umgeht, die bei der Arbeit mit GitHub auftreten.\nWie man mit Merge-Konflikten in GitHub umgeht.\n\n\n\nVorausetzungen\n\nVertrautheit mit GitHub.\nGit installiert haben.\nEin GitHub Konto haben.\n\n\n\nVersionskonflikte was ist das?\nVersionskonflikte entstehen normalerweise, wenn verschiedene Versionen derselben Datei gleichzeitig in das Hauptrepository gepusht werden und die Priorisierung der Dateien nicht klar ist, also:\n\nwenn man sein persönliches GitHub-Repository aktualisiert (kein Pull vor Push).\nwenn mehrere Personen gleichzeitig an derselben Datei arbeiten\n\n\n\nPush & Pull Konflikte\nEin typisches Szenario ist, dass Du etwas online auf GitHub bearbeitest und diese Änderung nicht gleichzeitig oder später in Rstudio synchronisierst. Der Konflikt könnte z.B. sein, dass Du einen Tippfehler in der README korrigierst und vergisst, die aktuelle Version im Rstudio-Projekt zu aktualisieren.\n\nAlso immer pull vor push, sonst hat GitHub zwei verschiedene Änderungen gespeichert und weiß nicht, welche zu verwenden ist.\n\nEin komplizierterer Fall ist, wenn eine Änderung im Master-Repository gemacht wurde und jemand anderes in seinem Branch-/Fork-Repository ebenfalls eine Änderung an der gleichen Datei bzw. dem gleichen Inhalt gemacht hat. Wenn eine Pull-Anfrage gestellt wird, wird GitHub den Unterschied bemerken. Auch hier kann es sich um etwas so Einfaches handeln, wie zwei Personen, die die README auf unterschiedliche Weise aktualisieren, was GitHub dazu veranlasst, ein Problem zu melden.\nIn diesem Fall muss manuell entschieden werden, welche Variante Vorrang hat.\nWenn Du eine Änderung an Deinem GitHub-Repository vornimmst und es gibt einen Konflikt, zeigt Dir R an, dass Deine Version dem Haupt-Repository voraus ist, wenn Du Deine Änderung überträgst. Wenn Du dies siehst, bedeutet es, dass es einen Unterschied zwischen den Dateien gibt. Wenn Du versuchst zu pullen und es gibt ein Problem, wird GitHub Dir etwas sagen wie\n\nUpdates wurden abgelehnt, weil das entfernte Repository Arbeit enthält, die Du lokal nicht hast. Dies wird normalerweise durch ein anderes Repository verursacht, das auf die gleiche Referenz pusht.\n\nWenn diese Meldung erscheint, empfiehlt GitHub, dass Du einen Pull von Deinem Master-Repository durchführst, um den Fehler zu finden. Häufig erhältst Du die Fehlermeldung\n\nCONFLICT (content): Konflikt beim Zusammenführen in [Datei]. Automatisches Zusammenführen fehlgeschlagen; Konflikte lösen und dann das Ergebnis übertragen.\n\nDie Datei mit dem Problem wird dann in Ihrem RStudio geöffnet und zeigt den gefundenen Fehler an. Es wird angezeigt, welche Änderungen vorgenommen wurden und welche Unterschiede zum Hauptzweig bestehen (die Änderungen werden unter &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD angezeigt, der Inhalt des Hauptzweigs wird darunter angezeigt). Du musst den Fehler zwischen den beiden Versionen beheben, indem Du entweder das beibehältst, was GitHub bereits hat, oder indem Du Deine Änderung so anpasst, dass sie dem entspricht, was Du machen wolltest. Wenn Du mit Deiner Änderung zufrieden bist, rufe das Terminal auf (es befindet sich in R, einem Tab über der Konsole). Im Terminal gibst Du git add [Dateiname] ein, drückst die Eingabetaste und gehst zurück zum Git-Tab oben rechts im RStudio-Fenster. Wähle die Datei aus, in der der Fehler aufgetreten ist und überschreibe sie, um den Fehler zu beheben.\n\n\nMerge Konflikte\nWenn mehrere Personen am selben GitHub-Repository arbeiten oder Du nur einen Zweig verwendest, besteht die Möglichkeit, dass ein Merge-Konflikt auftritt. Zusammenführungskonflikte treten auf, wenn Änderungen am Haupt-Repository und an einem Zweig vorgenommen werden, die nicht übereinstimmen. Sobald eine Pull-Anfrage gestellt wird, muss der Eigentümer des Projektarchivs die Änderungen manuell überprüfen, sie können dann nicht automatisch zusammengeführt werden.\nFolglich teilt GitHub Dir mit, dass es die Versionen nicht automatisch zusammenführen kann, aber es wird Dir trotzdem erlauben, die Pull-Anfrage zu stellen. Wenn Du Dich entscheidest, die Pull-Anfrage zu senden, wird der Repo-Besitzer nicht in der Lage sein, auf den grünen Merge Button zu klicken, sondern er wird eine Meldung sehen, die besagt:\n\nDieser Zweig hat Konflikte, die gelöst werden müssen.\n\nRechts neben dieser Meldung befindet sich die Schaltfläche Konflikte auflösen.\nWenn du auf die Schaltfläche Konflikte auflösen klickst, wirst du zu einer Seite weitergeleitet, die ähnlich aussieht wie bei Push- oder Pull-Fehlernt. Du siehst die vorgeschlagenen Änderungen aus dem Zweig und Haupt-Repository. An dieser Stelle können dann Änderungen durchgeführt werden und zuletzt mit Als gelöst markieren und anschließend Merge bestätigen erfolgreich für einen Merge bereitgestellt werden. Zuletzt muss der Eigentümer auf Merge Pull Request und dann auf Commit Merge klicken, um die Änderung im Haupt-Repository zu vorzunehmen.\n\n\nSelbst-Check\n\n\n\n\n\nGut zu wissen\n\nWeitere Informationen über den Umgang mit Konflikten in GitHub findest Du hier:\n\nWie gehe ich mit Merge Konflikten um?"
  },
  {
    "objectID": "modules/de-git-module.html#rstudio---all-inclusive",
    "href": "modules/de-git-module.html#rstudio---all-inclusive",
    "title": "Git, GitHub & Rstudio [DE]",
    "section": "RStudio - All Inclusive",
    "text": "RStudio - All Inclusive\n\nLernziele\n\nEinsatz von GitHub direkt aus RStudio\n\n\n\nVorausetzungen\n\nÜbung im Umgang mit GitHub und git\n\n\n\nExistierendes GitHub Repo in R einbinden\nBevor du mit einem GitHub-Repository in RStudio arbeitest, stelle sicher, dass du ein GitHub-Repository hast, mit dem du arbeiten kannst.\nNachdem du das Repository erstellt hast, kannst du auf die grüne Schaltfläche klicken, um einen Link zu erhalten, mit dem du das Repository klonen kannst. Um es in R zu öffnen, öffne R und klicke auf den Würfel mit dem Pluszeichen, um ein neues Projekt zu erstellen, klicke auf Versionskontrolle und dann auf Git. Nun fügt man die zuvor kopierte URL ein und erstellt das Projekt. Jetzt hast du ein Projekt in R, das mit GitHub verbunden ist. Nun kannst du neue Dateien erstellen und sie auf GitHub hochladen, damit andere sie sehen können.\n\n\nErklärung der Schaltflächen/Befehle\nOben rechts (je nach Konfiguration von RStudio) befinden sich die Reiter Environment, History... Wähle die Registerkarte Git, um die Git-Befehle zu sehen. In diesem Bereich kannst Du entscheiden, welche Dateien hochgeladen/gelöscht, welche Änderungen übernommen, welche Dateien aus dem Haupt-Repository gezogen, welche Dateien in das Haupt-Repository geschoben werden sollen. Die vorgenommenen Änderungen werden hier überprüft und es können Branches erstellt oder geändert wrden. Sehen wir uns nun an, was die einzelnen Befehle/Schaltflächen bewirken.\n\nDiff Wenn du auf Diff klickst, öffnet sich ein neues Fenster in R. In diesem Fenster werden alle Dateien angezeigt, die sich geändert haben (im Vergleich zum Haupt-Repository) und auch die Änderungen, die du vorgenommen hast. Du kannst dieses Fenster auch verwenden, um die Änderungen zu übertragen und aus dem Haupt-Repository herauszuziehen.\nCommit Die Verwendung von Commit im kleineren Fenster ist ähnlich wie im Diff-Fenster, Du musst nur die Dateien auswählen, die Du ins Repository übertragen möchtest und dann die Änderungen committen.\nPull Pull ist ziemlich selbsterklärend, es zieht Dateien aus dem GitHub Repository. Es ist wichtig, Dateien vor dem Pushen zu ziehen, um mögliche Konflikte mit überlappenden Dateien zu vermeiden.\nPush Push schiebt die Dateien in das GitHub Repository. Diese Funktion wird verwendet, wenn Du die Änderungen an Deinen Dateien abgeschlossen hast und bereit bist, sie hochzuladen, damit andere die neuen Dateien ansehen können. Die Reihenfolge beim Hochladen dieser Dateien wäre: Änderungen übertragen, aus dem Repository ziehen und dann in das Repository pushen.\nHistory Das nächste Symbol ist eine kleine Uhr, die die Historie Deiner Arbeit darstellt. Sie zeigt die bisherigen Übertragungen und was bei jeder Übertragung geändert wurde.\nRevert, Ignore und Shell Diese Befehle findest Du in einem Dropdown-Menü, nachdem Du auf das Zahnrad neben der Uhr geklickt hast. Mit Revert kannst Du alle Änderungen rückgängig machen, mit Ignore kannst Du einen Gitignore einrichten (nützlich, um Dateien zu blockieren, die Du nicht hochladen willst) und mit Shell kannst Du Dein Terminal öffnen und dort Git-Befehle ausführen.\nBranches Das nächste Symbol steht für Zweige. Wenn Du auf dieses Symbol klickst, wirst Du gefragt, ob Du einen neuen Zweig erstellen möchtest. Wie Du im Modul Zweige des Toolkits gelernt hast, sind Zweige nützlich, um Änderungen zu testen, ohne dass sie sich auf den Hauptzweig auswirken, falls ein Fehler auftritt. Du kannst das Dropdown-Menü rechts neben dem Zweigsymbol verwenden, um zwischen den Zweigen zu wechseln.\nTerminal (optional) Du kannst diese GitHub-Befehle mit den RStudio-Befehlen ausführen, aber du kannst auch das Terminal in R verwenden, um das gleiche zu tun. Alle GitHub-Befehle sind in der Form “git _____” und Du kannst sie finden, indem Du “git” in Dein Terminal eingibst. Dies macht dasselbe wie das R-Panel, aber wenn Du mit dem Schreiben von Git-Befehlen in einem Terminal vertrauter bist, funktioniert es vielleicht besser für Dich.\n\n\nEin R-Projekt in ein GitHub-Repositorium verwandeln\nManchmal arbeitet man an einem Projekt in R und hat vergessen, ein GitHub-Repository dafür zu erstellen. In diesem Fall kann Ihnen das Paket usethis helfen, ein Repo aus RStudio heraus zu erstellen. Mit der Funktion usethis::use_git kann das aktuelle Projekt in ein GitHub Repo umgewandelt werden, so dass die Dateien hochgeladen werden können. - Wenn Du diese Funktion zum ersten Mal ausführst, wirst Du wahrscheinlich einen Fehler erhalten, da Du dafür ein Token von GitHub benötigst. Nach dem Aufruf von usethis::browse_github_token öffnet sich ein neues Fenster, in dem man aufgefordert wird, sich in seinen GitHub-Account einzuloggen. Nach dem Einloggen können Berechtigungen mit dem Token gesetzt und kopiert werden. Sobald du den Token kopiert hast, rufe usethis::edit_r_environment() auf und speichere deinen Token als “GITHUB_PAT=token”.\nSobald dein Token gesetzt und dein R zurückgesetzt ist, kannst du use_git benutzen und es wird Dich fragen, ob es okay ist, deine Dateien zu GitHub zu committen. Wenn du diese Frage bejahst, wirst du aufgefordert, dein RStudio-Fenster neu zu starten, um das Git-Fenster zu öffnen und deine Dateien hochzuladen. Nach dem Neustart von RStudio die geänderten Dateien (falls vorhanden) mit dem Diff-Button hochladen. Benutze nun usethis::use_github, um deine Dateien in ein GitHub-Repository zu senden. - use_github wird Dich fragen, ob Du einen ssh Schlüssel hast, was Du wahrscheinlich nicht hast, also wähle https. Dann wird man gefragt, ob Titel und Beschreibung akzeptabel sind. Wenn ja, kannst Du mit Ja antworten und die Datei auf GitHub hochladen!\n\n\n\nRStudio und GitHub\n\n\n\n\n\n\n\nSelbst-Check\n\n\n\n\n\nGut zu wissen\n\nWeitere Informationen zur Verwendung von GitHub in RStudio findest Du unter folgendem Link:\n\nDer Blog-Eintrag GitHub & Rstudio zeigt, wie man Git in RStudio benutzt und geht dabei besonders auf die Terminal-Befehle ein."
  },
  {
    "objectID": "modules/de-git-module.html#danksagung",
    "href": "modules/de-git-module.html#danksagung",
    "title": "Git, GitHub & Rstudio [DE]",
    "section": "Danksagung",
    "text": "Danksagung\nDas Tutorial basiert auf dem DoSStoolkit. Sowohl die Inhalte als auch die Self Assessments basieren dem Modul Git outta here von Mariam Walaa & Matthew Wankiewicz. Die Übersetzungen und Veränderungen vom Autor dieser Seite.\nDas Originalmodul kann mit dem folgenden R-Befehl aufgerufen werden.\n\nlearnr::run_tutorial(\"git_outta_here\", package = \"DoSStoolkit\")"
  },
  {
    "objectID": "mc_session/mc3.html",
    "href": "mc_session/mc3.html",
    "title": "Spatial Interpolation",
    "section": "",
    "text": "The use of quantitative methods, especially statistical methods, is of considerable importance for describing and explaining spatial patterns (e.g. landscape ecology). The central concept on which these methods are based is that of proximity, or location in relation to each other.",
    "crumbs": [
      "Microclimate Data Retrieval & Spatial Data Interpolation",
      "Spatial Interpolation"
    ]
  },
  {
    "objectID": "mc_session/mc3.html#distance-and-data-representation",
    "href": "mc_session/mc3.html#distance-and-data-representation",
    "title": "Spatial Interpolation",
    "section": "Distance and data representation",
    "text": "Distance and data representation\nLet’s take a closer look at proximity, which is mentioned frequently. What exactly is it? How can proximity/neighborliness be expressed in such a way that the space becomes meaningful?\nIn general, spatial relationships are described in terms of neighborhoods (positional) and distances (metric). In spatial analysis or prediction, however, it is important to be able to name the spatial influence, i.e. the evaluation or weighting of this relationship, either qualitatively or quantitatively. Tobler did this for a specific objective by stating that “near” is more important than “far”. But what about in other cases? The challenge is that spatial influence can only be measured directly in exceptional cases. There are many ways to estimate it, however.\n\nNeighborhood\nNeighborhood is perhaps the most important concept. Higher dimensional geo-objects can be considered neighboring if they touch each other, e.g. neighboring countries. For zero-dimensional objects (points), the most common approach is to use distance in combination with a number of points to determine neighborhood.\n\n\nDistance\nProximity or neighborhood analyses are often concerned with areas of influence or catchment areas, i.e. spatial patterns of effects or processes.\nThis section discusses some methods for calculating distances between spatial objects. Because of the different ways of discretizing space, we must make the – already familiar – distinction between vector and raster data models.\nInitially, it is often useful to work without spatially restrictive conditions in a first analysis, e.g. when this information is missing. The term “proximity” inherently implies a certain imprecision. Qualitative terms that can be used for this are: “near”, “far” or “in the neighborhood of”. Representation and data-driven analysis require these terms to be objectified and operationalized. So, this metric must be based on a distance concept, e.g. Euclidean distance or travel times. In a second interpretative step, we must decide which units define this type of proximity. In terms of the objective of a question, there are only suitable and less-suitable measures; there is no correct or incorrect. Therefore, it is critical to define a meaningful neighborhood relationship for the objects under investigation.",
    "crumbs": [
      "Microclimate Data Retrieval & Spatial Data Interpolation",
      "Spatial Interpolation"
    ]
  },
  {
    "objectID": "mc_session/mc3.html#filling-spatial-gaps",
    "href": "mc_session/mc3.html#filling-spatial-gaps",
    "title": "Spatial Interpolation",
    "section": "Filling spatial gaps",
    "text": "Filling spatial gaps\nNow that we have learned the basic concepts of distance, neighborhood and filling spatial gaps, let’s take a look at interpolating or predicting values in space.\nFor many decades, deterministic interpolation techniques (inverse distance weighting, nearest neighbor, kriging) have been the most popular spatial interpolation techniques. External drift kriging and regression kriging, in particular, are fundamental techniques that use spatial autocorrelation and covariate information, i.e. sophisticated regression statistics.\nMachine learning algorithms like random forest have become very popular for spatial environmental prediction. One major reason for this is that they are can take into account non-linear and complex relationships, i.e. compensate for certain disadvantages that are present in the usual regression methods.",
    "crumbs": [
      "Microclimate Data Retrieval & Spatial Data Interpolation",
      "Spatial Interpolation"
    ]
  },
  {
    "objectID": "mc_session/mc3.html#proximity-concepts",
    "href": "mc_session/mc3.html#proximity-concepts",
    "title": "Spatial Interpolation",
    "section": "Proximity concepts",
    "text": "Proximity concepts\n\nVoronoi polygons – dividing space geometrically\nVoronoi polygons (aka Thiessen polygons) are an elementary method for geometrically determining proximity or neighborhoods. Voronoi polygons (see figure below) divide an area into regions that are closest to a given point in a set of irregularly distributed points. In two dimensions, a Voronoi polygon encompasses an area around a point, such that every spatial point within the Voronoi polygon is closer to this point than to any other point in the set. Such constructs can also be formed in higher dimensions, giving rise to Voronoi polyhedra.\n&lt;/frame&gt;\n&lt;iframe width=\"780\" height=\"500\" src=\"https://geomoer.github.io/geoAI//assets/images/unit01/suisse6.html\" title=\"Interpol\"&gt;\n\n\nThe blue dots are a typical example of irregularly distributed points in space – in this case, rain gauges in Switzerland. The overlaid polygons are the corresponding Voronoi segments that define the corresponding closest geometrical areas (gisma 2021)“\n\n\nSince Voronoi polygons correspond to an organizational principle frequently observed in both nature (e.g. plant cells) and in the spatial sciences (e.g. central places , according to Christaller), there are manifold possible applications. Two things must be assumed, however: First, that nothing else is known about the space between the sampled locations and, second, that the boundary line between two samples is incomplete idea.\nVoronoi polygons can also be used to delineate catchment areas of shops, service facilities or wells, like in the example of the Soho cholera outbreak. Please note that within a polygon, one of the spatial features is isomorphic, i.e. the spatial features are identical.\nBut what if we know more about the spatial relationships of the features? Let’s have a look at some crucial concepts.\n\n\nSpatial interpolation of data\nSpatially interpolating data points provides us with a modeled quasi-continuous estimation of features under the corresponding assumptions. But what is spatial interpolation? Essentially, this means using known values to calculate neighboring values that are unknown. Most of these techniques are among the most complex methods of spatial analysis, so we will deliberately limit ourselves here to a basic overview of the methods. Some of the best-known and common interpolation methods found in spatial sciences are nearest neighbor inverse distance, spline interpolations, kriging, and regression methods.\n\n\nContinously filling the gaps by interpolation\nTo get started, take a look at the following figure, which shows six different interpolation methods to derive the spatial distribution of precipitation in Switzerland (in addition to the overlaid Voronoi tessellation).\n\n\n\nThe blue dots are a typical example of irregularly distributed points in space – in this case, rain gauges in Switzerland. The size of each dot corresponds to the amount of precipitation in mm. The overlaid polygons are the corresponding Voronoi segments that define the corresponding closest geometrical areas (gisma 2021)” top left: Nearest neighbor interpolation based on 3-5 nearest neighbors, top right: Inverse Distance weighting (IDW) interpolation method middle left: AutoKriging with no additional parameters, middle right: Thin plate spline regression interpolation method bottom left: Triangular irregular net (TIN) surface interpolation, bottom right: additive model (GAM) interpolation\n\n\nIn the example of precipitation in Switzerland, the positions of the weather stations are fixed and cannot be freely chosen.\nWhen choosing an appropriate interpolation method, we need to pay attention to several properties of the samples (distribution and properties of the measurement points):\n\nRepresentativeness of measurement points: The sample should represent the phenomenon being analyzed in all of its manifestations.\nHomogeneity of measurement points: The spatial interdependence of the data is a very important basic requirement for further meaningful analysis.\nSpatial distribution of measurement points: The spatial distribution is of great importance. It can be completely random, regular or clustered.\nNumber of measurement points: The number of measurement points depends on the phenomenon and the area. In most cases, the choice of sample size is subject to practical limitations.\n\nWhat makes things even more complex is that these four factors – representativeness, homogeneity, spatial distribution and size – are all interrelated. For example, a sample size of 5 measuring stations for estimating precipitation for all of Switzerland is hardly meaningful and therefore not representative. Equally unrepresentative would be selecting every measuring station in German-speaking Switzerland to estimate precipitation for the entire country. In this case, the number alone might be sufficient, but the spatial distribution would not be. If we select every station at an altitude below 750 m asl, the sample could be correct in terms of both size and spatial distribution, but the phenomenon is not homogeneously represented in the sample. An estimate based on this sample would be clearly distorted, especially in areas above 750 m asl. In practice, virtually every natural spatially-continuous phenomenon is governed by stochastic fluctuations, so, mathematically speaking, it can only be described in approximate terms.\n\n\nMachine learning\nMachine learning (ML) methods such as random forest can also produce spatial and temporal predictions (i.e. produce maps from point observations). These methods are particularly robust because they take spatial autocorrelation into account, which can improve predictions or interpolations by adding geographic distances. This ultimately leads to better maps with much more complex relationships and dependencies.\nIn the simplest case, the results are comparable to the well-known model-based geostatistics. The advantage of ML methods over model-based geostatistics, however, is that they make fewer assumptions, can take non-linearities into account and are easier to automate.\n\n\n\nThe original dataset (top left) is a terrain model reduced to 8 meters with 48384 single pixels. For interpolation, 1448 points were randomly drawn and interpolated with conventional kriging (top right), support vector machines (SVM) (middle left), neural networks (middle right), and two variants of random forest (bottom row). In each method, only the distance of the drawn points is used as a dependency.\n\n\n\nEach interpolation method was applied using the “default” settings. Tuning could possibly lead to significant changes in all of them. Fascinatingly, the error measures correlate to the visual results: Kriging and the neural network show the best performance, followed by the random forest models and the support-vector machine.\n\n\n\nmodel\ntotal_error\nmean_error\nsd_error\n\n\n\n\nKriging\n15797773.0\n54.2\n67.9\n\n\nNeural Network\n19772241.0\n67.8\n80.5\n\n\nRandom Forest\n20540628.1\n70.4\n82.5\n\n\nNormalized Random Forest\n20597969.8\n70.6\n82.7\n\n\nSupport Vector Machine\n21152987.7\n72.5\n68.3\n\n\n\n\n\nAdditional references\nGet the Most Out of AI, Machine Learning, and Deep Learning Part 1 (10:52) and Part 2 (13:18)\nWhy You Should NOT Learn Machine Learning! (6:17)\nGeoAI: Machine Learning meets ArcGIS (8:50)",
    "crumbs": [
      "Microclimate Data Retrieval & Spatial Data Interpolation",
      "Spatial Interpolation"
    ]
  },
  {
    "objectID": "mc_session/mc3.html#hands-on-our-data",
    "href": "mc_session/mc3.html#hands-on-our-data",
    "title": "Spatial Interpolation",
    "section": "Hands on our data",
    "text": "Hands on our data\n\nSetup the environment\nPlease download the data from the repository or take the USB-stick\n\n#------------------------------------------------------------------------------\n# Author: creuden@gmail.com\n# Description:  interpolates the air temp\n# Copyright:GPL (&gt;= 3)  Date: 2023-08-28 \n#------------------------------------------------------------------------------\n\n# 0 ---- project setup ----\n\n# load packages (if not installed please install via install.packages())\nlibrary(\"raster\")\nlibrary(\"terra\")\nlibrary(\"sp\")\nlibrary(\"sf\")\nlibrary(\"dplyr\")\nlibrary(\"lwgeom\")\nlibrary(\"readxl\")\nlibrary(\"highfrequency\")\nlibrary(\"tidyverse\")\nlibrary(\"rprojroot\")\nlibrary(\"tibble\")\nlibrary(\"xts\")\nlibrary(\"data.table\")\nlibrary(\"mapview\")\nlibrary(stars)\nlibrary(gstat)\n\n# create a string containing the current working directory\nwd=paste0(find_rstudio_root_file(),\"/mc_session/data/\")\n\n# define time period to aggregate temp dat\ntime_period = 3\n\n# multiplication factor for blowing up the Copernicus DEM\nblow_fac = 15\n\n# reference system as proj4 string for old SP package related stuff\ncrs = raster::crs(\"+proj=utm +zone=33 +datum=WGS84 +units=m +no_defs\")\nsfcrs &lt;- st_crs(\"EPSG:32633\")\n\n# Copernicus DEM (https://land.copernicus.eu/imagery-in-situ/eu-dem/eu-dem-v1.1)\nfnDTM = paste0(wd,\"copernicus_DEM.tif\")  \n\n# Weather Data adapt if you download a new file \n# https://www.ecowitt.net/home/index?id=20166)\n# https://www.ecowitt.net/home/index?id=149300\nfn_dataFC29 = paste0(wd,\"all_GW1000A-WIFIFC29.xlsx\")\nfn_dataDB2F =paste0(wd,\"all_GW1000A-WIFIDB2F.xlsx\")\n\n# station data as derived by the field group\nfn_pos_data= paste0(wd,\"stations_prelim.shp\")\n\n# arbitrary plot borders just digitized for getting a limiting border of the plot area\nfn_area =paste0(wd,\"plot.shp\")\n\n# rds file for saving the cleaned up weather data\ncleandata = paste0(wd,\"climdata.RDS\")\n\n# 1 ---- read data ----\n# read_sf(\"data/de_nuts1.gpkg\") |&gt; st_transform(crs) -&gt; de\n# read DEM data\nDTM = terra::rast(fnDTM) # DTM.\n# increase resolution by 15\nDTM=disagg(DTM, fact=c(blow_fac, blow_fac)) \n#rename layer to altitude\nnames(DTM)=\"altitude\"\nr=DTM*0\n\n# read station position data\n pos=st_read(fn_pos_data)\n\nReading layer `stations_prelim' from data source \n  `/home/creu/edu/gisma-courses/EON2024/mc_session/data/stations_prelim.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 14 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 183055.8 ymin: 5748366 xmax: 183170.3 ymax: 5748499\nProjected CRS: WGS 84 / UTM zone 33N\n\n # read station position data\narea=st_read(fn_area)\n\nReading layer `plot' from data source \n  `/home/creu/edu/gisma-courses/EON2024/mc_session/data/plot.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1 feature and 1 field\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 10.40154 ymin: 51.79506 xmax: 10.40658 ymax: 51.79803\nGeodetic CRS:  WGS 84\n\n# reproject the dataset to the project crs\narea=st_transform(area,crs)\n# read temperature data we need to skip row 1 due to excel format\nclim_dataFC29 = as_tibble(read_excel(fn_dataFC29, skip = 1)) \nclim_dataDB2F = as_tibble(read_excel(fn_dataDB2F, skip = 1))\n\n\n\nCleaning data\nWe need to do an ugly cleaning job. This is basically the most cumbersome part of dealing with data analysis.\n\n# select the required cols\ntempFC29 = clim_dataFC29 %&gt;% dplyr::select(c(1,2,32,36,40,44,48))\ntempDB2F = clim_dataDB2F %&gt;% dplyr::select(c(1,25,29,33,37,41,45,49,53))\n# rename header according to the pos file names and create a merge field time\nnames(tempDB2F) = c(\"time\",\"ch1_r\",\"ch2_r\",\"ch3_r\",\"ch4_r\",\"ch5_r\",\"ch6_r\",\"ch7_r\",\"ch8_r\")\nnames(tempFC29) = c(\"time\",\"base\",\"ch1\",\"ch2\",\"ch3\",\"ch4\",\"ch5\")\n#merge files\ntemp=merge(tempFC29,tempDB2F)\n# convert datum which is a string to date format\ntemp$time=as.POSIXct(temp$time)\n# aggregate timeslots according to the value in time_period\ntemp3h = aggregateTS(as.xts(temp), alignBy = \"hours\",dropna = T,alignPeriod = time_period)\n# add the datum colum (which is now a pointer of the timeseries) as first col in the dataset\ntemp_fin=as_tibble(temp3h) %&gt;% add_column(time = index(temp3h), .before = 1)\n# transpose and combine the table\ntemp_fin=as_tibble(cbind(nms = names(temp_fin), t(temp_fin)))\n# delete first row \nnames(temp_fin) = temp_fin[1,]\ntemp_fin=temp_fin[-1,]\n# replace names specially time by stationid\nnames(temp_fin)[names(temp_fin) == 'time'] = 'stationid'\n# extract altitudes for positions\npos$altitude= exactextractr::exact_extract(DTM,st_buffer(pos,1),\"mean\")\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |======================================================================| 100%\n\n# merge positions and values via id\nm=merge(pos,temp_fin)\n# make the var name working for gstat by replacing all patterns\nn= gsub(x = names(m),pattern = \"-\",replacement = \"\")\nn= gsub(x = n,pattern = \" \",replacement = \"\")\nn= gsub(x = n,pattern = \":\",replacement = \"\")\nn= gsub(x = n,pattern = \"2023\",replacement = \"A2023\")\n# and rename couse this as new names\nnames(m)=n\nm= st_transform(m,sfcrs)\n\nsaveRDS(m,cleandata)\n\n\n\nPreparing and converting spatial basis data sets\nAfter the basic cleaning is finished we prepare some specific datasets according to the technical needs.\n\n# grep the varnames for an interpolation loop\nvars=grep(glob2rx(\"A2023*\"), n, value = TRUE)\nvars\n\n[1] \"A20230828230000\" \"A20230829020000\" \"A20230829050000\" \"A20230829080000\"\n[5] \"A20230829110000\" \"A20230829140000\" \"A20230829170000\" \"A20230829200000\"\n[9] \"A20230829220000\"\n\n# convert final sf vector to terra vector\ntemperature_vect = vect(m)\ntemperature_vect \n\n class       : SpatVector \n geometry    : points \n dimensions  : 14, 11  (geometries, attributes)\n extent      : 183055.8, 183170.3, 5748366, 5748499  (xmin, xmax, ymin, ymax)\n coord. ref. : WGS 84 / UTM zone 33N (EPSG:32633) \n names       : stationid altitude A20230828230000 A20230829020000\n type        :     &lt;chr&gt;    &lt;num&gt;           &lt;chr&gt;           &lt;chr&gt;\n values      :      base    579.1            10.3            10.1\n                     ch1      578            10.6            10.2\n                   ch1_r    575.3            10.7            10.0\n A20230829050000 A20230829080000 A20230829110000 A20230829140000\n           &lt;chr&gt;           &lt;chr&gt;           &lt;chr&gt;           &lt;chr&gt;\n             9.5             9.6            14.8              13\n             9.5             9.7              12            14.3\n             9.6            10.4            13.4            18.3\n A20230829170000 A20230829200000 A20230829220000\n           &lt;chr&gt;           &lt;chr&gt;           &lt;chr&gt;\n            15.2            12.8            11.5\n            16.9            12.9            10.4\n            19.2             9.9             8.1\n\n# create table containing x, y, value (A20230829220000) to interpolate this values in space\nxyz=cbind(geom(temperature_vect)[,3],geom(temperature_vect)[,4],as.numeric(temperature_vect$A20230829220000))\n# convert to data frame and name header\nxyz=data.frame(xyz)\nnames(xyz) =c(\"x\",\"y\",\"temp\")\nxyz\n\n          x       y temp\n1  183137.7 5748385 11.5\n2  183087.0 5748438 10.4\n3  183130.3 5748437  8.1\n4  183055.8 5748460 10.7\n5  183098.0 5748374  9.7\n6  183081.0 5748499 10.6\n7  183110.5 5748389  9.7\n8  183095.5 5748465 10.7\n9  183144.5 5748366  8.9\n10 183120.1 5748476 10.5\n11 183170.3 5748415  6.4\n12 183144.8 5748427  7.4\n13 183156.8 5748389  7.4\n14 183134.0 5748399  7.8\n\n# the same just for x,y\nxy=cbind(geom(temperature_vect)[,3],geom(temperature_vect)[,4])\n#the same just for z\nz=as.numeric(temperature_vect$A20230829220000)",
    "crumbs": [
      "Microclimate Data Retrieval & Spatial Data Interpolation",
      "Spatial Interpolation"
    ]
  },
  {
    "objectID": "mc_session/mc3.html#further-hands-on-examples",
    "href": "mc_session/mc3.html#further-hands-on-examples",
    "title": "Spatial Interpolation",
    "section": "Further Hands on examples",
    "text": "Further Hands on examples\nThe Forgenius Pinus Pinaster Project provides an fully integrated GIS source code and field data dealing with prediction classificaten of UAV and station realted data.",
    "crumbs": [
      "Microclimate Data Retrieval & Spatial Data Interpolation",
      "Spatial Interpolation"
    ]
  },
  {
    "objectID": "tdv_session/02_ValidationDataCollection.html",
    "href": "tdv_session/02_ValidationDataCollection.html",
    "title": "Collection of validation data in the context of remote sensing based forest monitoring",
    "section": "",
    "text": "library(sf)\n\nLinking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.3.1; sf_use_s2() is TRUE\n\nlibrary(raster)\n\nLade nötiges Paket: sp\n\nlibrary(ggplot2)\nlibrary(rprojroot)\nwd=paste0(find_rstudio_root_file(),\"/tdv_session/data/\")\n\n\nIntroduction\nIn this tutorial we will explore the principles of design-based sampling. The simulation part is based on a presentation of Gerad Heuveling from Wageningen University, which he gave in the OpenGeoHub Summer School[https://opengeohub.org/summer-school/ogh-summer-school-2021/].\n\nLearn how to draw a spatial random sample\nLearn how to draw a systematic grid for a given area of interest\nRun a simulation for design-based sampling\n\n\n\nData sets\nFor demonstration purposes we will work with a map of forest above ground biomass (AGB) produced by the Joint Research Center(JRC) for the European Union European Commission (Joint Research Centre (JRC) (2020) http://data.europa.eu/89h/d1fdf7aa-df33-49af-b7d5-40d226ec0da3.)\nTo provide a synthetic example we will assume that this map (agb_pop) is an error free representation of the population. Additionally we use a second map (agb_model) compiled using a machine learning model (RF) also depicting the AGB distribution.\n\nnp_boundary = st_transform(st_read(paste0(wd,\"nlp-harz_aussengrenze.gpkg\")),25832)\n\nReading layer `nlp-harz_aussengrenze' from data source \n  `/home/creu/edu/gisma-courses/EON2024/tdv_session/data/nlp-harz_aussengrenze.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1 feature and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 591196.6 ymin: 5725081 xmax: 619212.6 ymax: 5751232\nProjected CRS: WGS 84 / UTM zone 32N\n\nagb_pop &lt;- raster(paste0(wd,\"agb_np_harz_truth.tif\"))\n\nagb_model &lt;-raster(paste0(wd,\"agb_np_harz_model.tif\"))\n\nIf we assume the \\(z(x_i)=\\) agb.pop to be an exact representation of the population we can calculate the Root mean Square Error (RMSE) as the difference between the model predictions \\(\\hat{z(x_i)}\\) and the population map with:\n\\[\nRMSE = \\sqrt{\\frac{1}{N}\\sum{(z(x_{ctor. Also today there was wind, not good for m3 i})-\\hat{z}(x_{i}))^2}}\n\\]\n\nRMSE_pop = sqrt(cellStats((agb_pop-agb_model)^2, mean))\n\nBy looking at the difference from the “true” AGB and the difference we get a true RMSE of 41.23 t/ha.\n\n\nCollect a random sample\nSince we know the true RMSE, we can test if a random sample estimate has a similar RMSE. We start with a random sample with \\(n=100\\) sample points.\n\nn=100\np1 = st_sample(np_boundary,size=n)\nplot(st_geometry(np_boundary))\nplot(p1,add=TRUE,pch=1)\n\n\n\n\n\n\n\n\nWe can now extract the population values and the model values at the sample locations and calculate the RMSE for all sample points.\n\nsample &lt;- raster::extract((agb_pop-agb_model),as_Spatial(p1))\nRMSE_est &lt;- sqrt(mean((sample)^2,na.rm=T))\n\nThe random sample estimates the RMSE as 42.31.\nBut is this an unbiased estimate?\n\n\nSimulation of many random samples\nTo check if our sample based estimates are unbiased we will repeat the sampling \\(k\\) times.\n\ndif &lt;- as((agb_pop-agb_model), 'SpatialGridDataFrame')\nseed&lt;- 12324\n\n\nk &lt;- 500\nn &lt;- 50\nRMSE &lt;- rep(0,k) \n\nfor (i in 1:k) {\n  #print(i)\n  p1 = spsample(as_Spatial(np_boundary),n=n,type='random')\n  crs(p1)&lt;-crs(dif)\n  #sample &lt;- raster::extract((agb_pop-agb_model),p1)\n  error&lt;-over(p1,dif)$layer\n  RMSE[i] &lt;- sqrt(mean((error)^2,na.rm=T))\n}\n\ndf &lt;- data.frame(x=RMSE, y=rep('a',k))\n\nggplot(data=df,aes(x=x))+\n  geom_density(data=subset(df,y=='a'),\n               fill='blue', alpha=0.5)+\n  xlab('RMSE')+geom_vline(xintercept=RMSE_pop,linewidth=1.5,\n                          color ='black', linetype='longdash')+\n  geom_vline(xintercept=mean(df$x),size=1.5,\n                          color ='black')\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nWe see that the true RMSE and the mean of the \\(k\\) simulation runs are almost equal. Thus, we can assume an unbiased estimate of the RMSE.\nBut how does the sample size \\(n\\) affects the accuracy?\n\nk &lt;- 500\nn &lt;- 100\nRMSE_2 &lt;- rep(0,k) \n\nfor (i in 1:k) {\n  #print(i)\n  p1 = spsample(as_Spatial(np_boundary),n=n,type='random')\n  crs(p1)&lt;-crs(dif)\n  #sample &lt;- raster::extract((agb_pop-agb_model),p1)\n  error&lt;-over(p1,dif)$layer\n  RMSE_2[i] &lt;- sqrt(mean((error)^2,na.rm=T))\n}\n\ndf_2 &lt;- data.frame(x=RMSE_2, y=rep('b',k))\ndf&lt;-rbind(df,df_2)\n\nggplot(data=df,aes(x=x,fill=y))+\n  geom_density(alpha=0.5)+\n  scale_fill_discrete(labels=c('Random, n=50', 'Random, n=100'))+\n  xlab('RMSE')+geom_vline(xintercept=RMSE_pop,size=1.5,\n                          color ='black', linetype='longdash')+\n  geom_vline(xintercept=mean(df$x),size=1.5,\n                          color ='black')\n\n\n\n\n\n\n\n\nWe see that the precision of the esimtates is increased. How much did the uncertainty decrease when we increase the sample size from \\(n=50\\) to \\(n=100\\)?\n\nsd(RMSE_2)/sd(RMSE)\n\n[1] 0.6675957\n\n\n\n\nSystematic sampling\nInstead of a random sampling, systematic designs are more common in forest inventories for the following reasons:\n\nEasy to establish and to document\nEnsures a balanced spatial coverage\n\n\np1 = spsample(as_Spatial(np_boundary),n=n,type='regular')\n\nplot(np_boundary$geom)\nplot(p1, add=T)\n\n\n\n\n\n\n\n\n\nk &lt;- 500\nn &lt;- 100\nRMSE_3 &lt;- rep(0,k) \n\nfor (i in 1:k) {\n  #print(i)\n  p1 = spsample(as_Spatial(np_boundary),n=n,type='regular')\n  crs(p1)&lt;-crs(dif)\n  error&lt;-over(p1,dif)$layer\n  RMSE_3[i] &lt;- sqrt(mean((error)^2,na.rm=T))\n}\n\ndf_3&lt;- data.frame(x=RMSE_3, y=rep('c',k))\ndf&lt;-rbind(df,df_3)\n\nggplot(data=df,aes(x=x, fill=y))+\n  geom_density(alpha=0.5)+\n  scale_fill_discrete(labels=c('Random, n=50', 'Random, n=100','Systematic, n=100'))+\n  xlab('RMSE')+geom_vline(xintercept=RMSE_pop,size=1.5,\n                          color ='black', linetype='longdash')+\n  geom_vline(xintercept=mean(df$x),size=1.5,\n                       color ='black')",
    "crumbs": [
      "Training Data",
      "Collection of validation data in the context of remote sensing based forest monitoring"
    ]
  }
]